"""
Monkey patch for ParseHub to fix issues:
1. YtParser format selector: Invalid format causes Facebook/YouTube videos to fail
2. YtParser cookie handling: YtParser doesn't pass cookies to yt-dlp
3. BiliAPI anti-crawler: BiliAPI doesn't set Referer headers for API calls
4. XhsParser empty download list: XhsParser crashes when download_list is empty
"""


def patch_parsehub_yt_dlp():
    """
    Patch ParseHub's YtParser to:
    1. Use correct format selector
    2. Pass cookies from ParseConfig to yt-dlp
    3. Patch BiliAPI to add Referer headers for anti-crawler
    4. Patch XhsParser to handle empty download list gracefully
    """
    try:
        import logging
        import os
        import tempfile
        import re
        import httpx
        logger = logging.getLogger(__name__)

        from parsehub.parsers.base.yt_dlp_parser import YtParser
        from parsehub.provider_api.bilibili import BiliAPI
        from parsehub.parsers.parser.xhs_ import XhsParser

        logger.info("ğŸ”§ Starting ParseHub patch...")

        def fixed_extract_info(self, url):
            """Fixed _extract_info that passes cookies to yt-dlp and stores TikHub download URL"""
            import re
            import httpx
            from yt_dlp import YoutubeDL

            params = self.params.copy()

            # Add proxy if configured
            if self.cfg.proxy:
                params["proxy"] = self.cfg.proxy

            # JavaScript runtimeé…ç½®ï¼š
            # yt-dlpé»˜è®¤æ”¯æŒdenoï¼Œä¼šè‡ªåŠ¨æ£€æµ‹PATHä¸­çš„deno
            # ä¸éœ€è¦æ‰‹åŠ¨é…ç½®js_runtimes (Dockerfileå·²å®‰è£…denoå¹¶æ·»åŠ åˆ°PATH)

            # Add headers (Referer/Origin) for anti-crawler
            # yt-dlpéœ€è¦è¿™äº›headersæ‰èƒ½ç»•è¿‡å„å¹³å°çš„åçˆ¬è™«æ£€æµ‹
            # é‡è¦ï¼šä¸è¦è¦†ç›–params["http_headers"]ï¼Œè€Œæ˜¯æ›´æ–°ç°æœ‰headers
            # å‚è€ƒ: yt_dlp/YoutubeDL.py:742 - params['http_headers'] = HTTPHeaderDict(std_headers, self.params.get('http_headers'))
            url_lower = url.lower()

            # è·å–ç°æœ‰headersï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ç©ºdict
            http_headers = params.get("http_headers", {})
            if not isinstance(http_headers, dict):
                http_headers = {}

            # ä¸è®¾ç½®User-Agentï¼Œè®©yt-dlpä½¿ç”¨random_user_agent()ï¼ˆæ›´å¥½çš„åçˆ¬è™«ï¼‰
            # å‚è€ƒ: yt_dlp/utils/networking.py:162 - 'User-Agent': random_user_agent()

            # æ ¹æ®å¹³å°æ·»åŠ Refererå’ŒOriginï¼ˆè¿™äº›æ˜¯å¿…éœ€çš„åçˆ¬è™«headersï¼‰
            if "youtube.com" in url_lower or "youtu.be" in url_lower:
                http_headers.update({
                    "Referer": "https://www.youtube.com/",
                    "Origin": "https://www.youtube.com"
                })
                logger.info(f"ğŸŒ [Patch] Added YouTube headers (Referer/Origin)")
            elif "bilibili.com" in url_lower or "b23.tv" in url_lower:
                http_headers.update({
                    "Referer": "https://www.bilibili.com/",
                    "Origin": "https://www.bilibili.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Bilibili headers (Referer/Origin)")
            elif "twitter.com" in url_lower or "x.com" in url_lower:
                http_headers.update({
                    "Referer": "https://twitter.com/",
                    "Origin": "https://twitter.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Twitter headers (Referer/Origin)")
            elif "instagram.com" in url_lower:
                http_headers.update({
                    "Referer": "https://www.instagram.com/",
                    "Origin": "https://www.instagram.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Instagram headers (Referer/Origin)")
            elif "kuaishou.com" in url_lower:
                http_headers.update({
                    "Referer": "https://www.kuaishou.com/",
                    "Origin": "https://www.kuaishou.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Kuaishou headers (Referer/Origin)")
            elif "facebook.com" in url_lower or "fb.watch" in url_lower:
                http_headers.update({
                    "Referer": "https://www.facebook.com/",
                    "Origin": "https://www.facebook.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Facebook headers (Referer/Origin)")

            # æ›´æ–°paramsï¼ˆè€Œä¸æ˜¯è¦†ç›–ï¼‰
            params["http_headers"] = http_headers
            logger.info(f"ğŸ” [Patch] Final http_headers: {http_headers}")

            # YouTubeç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ä¸“ç”¨ä»£ç†å’Œcookieï¼ˆå¦‚æœé…ç½®ï¼‰
            # YouTube çš„ bot æ£€æµ‹éå¸¸ä¸¥æ ¼ï¼Œéœ€è¦ä½¿ç”¨ä»£ç†å’Œcookieç»•è¿‡
            if "youtube.com" in url.lower() or "youtu.be" in url.lower():
                youtube_proxy = os.getenv("YOUTUBE_PROXY")
                if youtube_proxy:
                    params["proxy"] = youtube_proxy
                    logger.info(f"ğŸŒ [Patch] Using YouTube proxy: {youtube_proxy[:30]}...")

                # YouTube Cookie æ”¯æŒï¼šä»ç¯å¢ƒå˜é‡è¯»å– cookie æ–‡ä»¶è·¯å¾„
                # Cookie å¯ä»¥å¸®åŠ© yt-dlp è§£æå…ƒæ•°æ®ï¼Œç»•è¿‡ç™»å½•éªŒè¯
                youtube_cookie_from_env = os.getenv("YOUTUBE_COOKIE")
                if youtube_cookie_from_env and "cookiefile" not in params:
                    logger.info(f"ğŸª [Patch] YouTube cookie from env: {youtube_cookie_from_env}")
                    if os.path.exists(youtube_cookie_from_env):
                        params["cookiefile"] = youtube_cookie_from_env
                        logger.info(f"ğŸª [Patch] Using YouTube cookie file: {youtube_cookie_from_env}")
                    else:
                        logger.warning(f"âš ï¸ [Patch] YouTube cookie file not found: {youtube_cookie_from_env}")

            # Add cookies if configured (FIX: YtParser doesn't handle cookies)
            # å‚è€ƒ: yt_dlp/YoutubeDL.py:349 - cookiefile: File name or text stream from where cookies should be read
            temp_cookie_file = None

            # å…¶ä»–å¹³å°cookieå¤„ç†ï¼ˆä»ParseConfigä¼ é€’ï¼‰
            # åªæœ‰åœ¨cookiefileè¿˜æ²¡è®¾ç½®æ—¶æ‰å¤„ç†
            if self.cfg.cookie and "cookiefile" not in params:
                logger.info(f"ğŸª [Patch] Received cookie type: {type(self.cfg.cookie)}, value preview: {str(self.cfg.cookie)[:100]}")
                # æ£€æŸ¥cookieç±»å‹ï¼šæ–‡ä»¶è·¯å¾„æˆ–å­—ç¬¦ä¸²
                if isinstance(self.cfg.cookie, str):
                    logger.info(f"ğŸª [Patch] Cookie is string, checking if file exists: {self.cfg.cookie}")
                    # åˆ¤æ–­æ˜¯æ–‡ä»¶è·¯å¾„è¿˜æ˜¯cookieå­—ç¬¦ä¸²
                    if os.path.exists(self.cfg.cookie):
                        logger.info(f"ğŸª [Patch] File exists! Setting cookiefile parameter")
                        # Netscapeæ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                        params["cookiefile"] = self.cfg.cookie
                        logger.info(f"ğŸª [Patch] Using cookie file: {self.cfg.cookie}")
                    else:
                        # Bilibili/Twitterç­‰cookieå­—ç¬¦ä¸²ï¼Œè§£æåå†™ä¸´æ—¶æ–‡ä»¶
                        logger.info(f"ğŸª [Patch] Parsing cookie string (len={len(self.cfg.cookie)})")

                        # è§£æcookieå­—ç¬¦ä¸²ä¸ºdict
                        cookie_dict = {}
                        for item in self.cfg.cookie.split(';'):
                            item = item.strip()
                            if '=' in item:
                                key, value = item.split('=', 1)
                                cookie_dict[key.strip()] = value.strip()

                        # æ ¹æ®URLåˆ¤æ–­domain
                        url_lower = url.lower()
                        if "bili" in url_lower:
                            domain = ".bilibili.com"
                        elif "twitter.com" in url_lower or "x.com" in url_lower:
                            domain = ".twitter.com"
                        elif "instagram.com" in url_lower:
                            domain = ".instagram.com"
                        elif "kuaishou.com" in url_lower:
                            domain = ".kuaishou.com"
                        else:
                            domain = ".example.com"

                        # å†™å…¥ä¸´æ—¶Netscapeæ ¼å¼æ–‡ä»¶
                        temp_cookie_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
                        temp_cookie_file.write("# Netscape HTTP Cookie File\n")
                        for key, value in cookie_dict.items():
                            temp_cookie_file.write(f"{domain}\tTRUE\t/\tFALSE\t0\t{key}\t{value}\n")
                        temp_cookie_file.close()

                        params["cookiefile"] = temp_cookie_file.name
                        logger.info(f"ğŸª [Patch] Created temp cookie file for {domain}")

            try:
                with YoutubeDL(params) as ydl:
                    result = ydl.extract_info(url, download=False)

                # æ¸…ç†ä¸´æ—¶cookieæ–‡ä»¶
                if temp_cookie_file and os.path.exists(temp_cookie_file.name):
                    os.unlink(temp_cookie_file.name)

                return result
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶cookieæ–‡ä»¶
                if temp_cookie_file and os.path.exists(temp_cookie_file.name):
                    os.unlink(temp_cookie_file.name)
                error_msg = f"{type(e).__name__}: {str(e)}"
                raise RuntimeError(error_msg) from None

        # Apply YtParser patches
        # Note: Don't patch params property - it breaks subtitle configs and other settings
        # Only patch _extract_info method which handles js_runtimes internally
        YtParser._extract_info = fixed_extract_info
        logger.info("âœ… YtParser patched: js_runtimes + cookie handling + headers")

        # Note: YtParser._parse doesn't need patching anymore - using original implementation
        # YouTube downloads will be handled by pytubefix in the download method
        logger.info("â„¹ï¸ YtParser._parse: using original implementation (YouTube download via pytubefix)")

        # Patch YtVideoParseResult.download to use pytubefix for YouTube
        from parsehub.parsers.base.yt_dlp_parser import YtVideoParseResult
        from parsehub.types import DownloadResult, Video
        from parsehub.config import DownloadConfig
        from parsehub.types.error import DownloadError
        from pathlib import Path
        import time
        import asyncio

        original_yt_video_download = YtVideoParseResult.download

        async def patched_yt_video_download(self, path=None, callback=None, callback_args=(), config=DownloadConfig()):
            """Patched download that uses pytubefix for YouTube"""
            logger.info(f"ğŸ” [Patch] patched_yt_video_download called: is_url={self.media.is_url}, path={self.media.path[:100] if self.media.path else 'None'}")

            if not self.media.is_url:
                logger.info(f"âš ï¸ [Patch] media.is_url is False, returning media directly")
                return self.media

            # Check if this is a YouTube URL
            url_lower = self.media.path.lower() if self.media.path else ""
            is_youtube = any(domain in url_lower for domain in ['youtube.com', 'youtu.be'])

            if is_youtube:
                logger.info(f"ğŸ“¥ [Patch] Detected YouTube URL, using pytubefix: {self.media.path[:80]}...")

                # Download directory
                dir_ = (config.save_dir if path is None else Path(path)).joinpath(f"{time.time_ns()}")
                dir_.mkdir(parents=True, exist_ok=True)

                if callback:
                    await callback(0, 0, "æ­£åœ¨ä¸‹è½½...", *callback_args)

                try:
                    # Use pytubefix to download
                    from pytubefix import YouTube

                    def download_with_pytubefix():
                        """Synchronous function to download with pytubefix"""
                        # Check if YouTube proxy is configured
                        youtube_proxy = os.getenv("YOUTUBE_PROXY")
                        proxies = None
                        if youtube_proxy:
                            # Parse proxy URL to dict format for pytubefix
                            # pytubefix expects: {'http': 'proxy_url', 'https': 'proxy_url'}
                            proxies = {
                                'http': youtube_proxy,
                                'https': youtube_proxy
                            }
                            logger.info(f"ğŸŒ [pytubefix] Using YouTube proxy: {youtube_proxy[:30]}...")

                        # Check if OAuth token is configured
                        youtube_oauth_token = os.getenv("YOUTUBE_OAUTH_TOKEN")
                        use_oauth = False
                        token_file = None

                        if youtube_oauth_token and os.path.exists(youtube_oauth_token):
                            use_oauth = True
                            token_file = youtube_oauth_token
                            logger.info(f"ğŸ” [pytubefix] Using YouTube OAuth token: {youtube_oauth_token}")

                        # Use 'WEB' client to enable automatic po_token generation
                        # This bypasses YouTube's bot detection without manual token extraction
                        # nodejs dependency is automatically installed via nodejs-wheel-binaries
                        # OAuth can be used as alternative to proxy (but requires Google account)
                        yt = YouTube(
                            self.media.path,
                            client='WEB',
                            proxies=proxies,
                            use_oauth=use_oauth,
                            allow_oauth_cache=True,
                            token_file=token_file
                        )

                        # Get highest resolution progressive stream (video + audio)
                        stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()

                        if not stream:
                            # Fallback to highest resolution stream
                            stream = yt.streams.get_highest_resolution()

                        if not stream:
                            raise DownloadError("No suitable stream found")

                        # Download to directory
                        logger.info(f"ğŸ¬ [pytubefix] Downloading: {yt.title} ({stream.resolution})")
                        output_path = stream.download(output_path=str(dir_), filename=f"video_{time.time_ns()}.mp4")

                        return output_path, yt

                    # Run in thread to avoid blocking
                    output_path, yt = await asyncio.to_thread(download_with_pytubefix)

                    logger.info(f"âœ… [Patch] pytubefix download completed: {output_path}")

                    return DownloadResult(
                        self,
                        Video(
                            path=str(output_path),
                            thumb_url=yt.thumbnail_url if hasattr(yt, 'thumbnail_url') else None,
                            height=0,  # pytubefix doesn't provide these easily
                            width=0,
                            duration=yt.length if hasattr(yt, 'length') else 0,
                        ),
                        dir_,
                    )
                except Exception as e:
                    logger.error(f"âŒ [Patch] pytubefix download failed: {e}, falling back to yt-dlp")
                    # Fallback to original yt-dlp download
                    return await original_yt_video_download(self, path, callback, callback_args, config)
            else:
                # Not a YouTube URL, use original yt-dlp download
                return await original_yt_video_download(self, path, callback, callback_args, config)

        YtVideoParseResult.download = patched_yt_video_download
        logger.info("âœ… YtVideoParseResult.download patched: use pytubefix for YouTube")

        # Patch BiliAPI to support cookies and add Referer headers
        # Problem: BiliAPI.__init__ doesn't accept cookie parameter
        # Solution: Patch __init__ to accept cookie, and patch get_video_info to use it
        original_bili_init = BiliAPI.__init__
        original_get_video_info = BiliAPI.get_video_info

        def patched_bili_init(self, proxy: str = None, cookie: dict = None):
            """Patched BiliAPI.__init__ to accept cookie parameter"""
            original_bili_init(self, proxy)
            # ä¿å­˜cookieä¾›APIè°ƒç”¨ä½¿ç”¨
            self.cookie = cookie

            # æ·»åŠ å¿…è¦çš„headers
            from yt_dlp.utils.networking import random_user_agent
            self.headers.update({
                "Referer": "https://www.bilibili.com/",
                "Origin": "https://www.bilibili.com",
                "User-Agent": random_user_agent()
            })
            if cookie:
                logger.info(f"ğŸŒ [Patch] BiliAPI initialized with cookie and anti-crawler headers")
            else:
                logger.info(f"ğŸŒ [Patch] BiliAPI initialized with anti-crawler headers (no cookie)")

        async def patched_get_video_info(self, url: str):
            """Patched get_video_info to use self.cookie"""
            bvid = self.get_bvid(url)
            # ä½¿ç”¨self.cookieè€Œä¸æ˜¯ç¡¬ç¼–ç None
            response = await self._get_client().get(
                "https://api.bilibili.com/x/web-interface/view/detail",
                params={"bvid": bvid},
                cookies=self.cookie  # ä¼ å…¥cookieï¼
            )
            return response.json()

        BiliAPI.__init__ = patched_bili_init
        BiliAPI.get_video_info = patched_get_video_info

        # Note: BiliParse.bili_api_parse creates BiliAPI without cookie
        # But since we patched BiliAPI.__init__ to accept cookie parameter,
        # we need to ensure cookie is passed. The easiest way is to patch
        # the BiliAPI creation call in BiliParse, but that's complex.
        #
        # Instead, we rely on the fact that ParseConfig.cookie is accessible
        # via self.cfg.cookie in BiliParse. We just need BiliParse to pass it.
        #
        # Since we can't easily modify the calling code, we make BiliAPI
        # read cookie from environment if not provided in __init__.

        # Update: Simplify - patch BiliAPI to read from environment variable
        original_bili_init_v2 = BiliAPI.__init__

        def patched_bili_init_v2(self, proxy: str = None, cookie: dict = None):
            """Enhanced BiliAPI.__init__ that reads cookie from env if not provided"""
            # å¦‚æœæ²¡ä¼ cookieï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
            if not cookie:
                import os
                cookie_str = os.getenv("BILIBILI_COOKIE")
                if cookie_str:
                    cookie = {}
                    for item in cookie_str.split(';'):
                        item = item.strip()
                        if '=' in item:
                            key, value = item.split('=', 1)
                            cookie[key.strip()] = value.strip()
                    logger.info(f"ğŸŒ [Patch] BiliAPI loaded cookie from environment")

            # è°ƒç”¨ä¹‹å‰patchçš„ç‰ˆæœ¬
            patched_bili_init(self, proxy, cookie)

        BiliAPI.__init__ = patched_bili_init_v2
        logger.info("âœ… BiliAPI patched: cookie support (from env) + anti-crawler headers")

        # Patch XhsParser to handle empty download list
        # Reference: parsehub/parsers/parser/xhs_.py - parse method line 15
        original_xhs_parse = XhsParser.parse

        async def patched_xhs_parse(self, url: str):
            """Patched XhsParser.parse to handle empty download list"""
            from parsehub.types import VideoParseResult, ImageParseResult, MultimediaParseResult, Video, Image
            from parsehub.parsers.parser.xhs_ import XHS, Log

            # è°ƒç”¨åŸå§‹é€»è¾‘è·å–æ•°æ®
            url = await self.get_raw_url(url)
            async with XHS(user_agent="", cookie="") as xhs:
                x_result = await xhs.extract(url, False, log=Log)

            from parsehub.types.error import ParseError
            if not x_result or not (result := x_result[0]):
                raise ParseError("å°çº¢ä¹¦è§£æå¤±è´¥")

            desc = self.hashtag_handler(result["ä½œå“æè¿°"])
            k = {"title": result["ä½œå“æ ‡é¢˜"], "desc": desc, "raw_url": url}

            # Livephotoå¤„ç†
            if all(result["åŠ¨å›¾åœ°å€"]):
                return MultimediaParseResult(media=[Video(i) for i in result["åŠ¨å›¾åœ°å€"]], **k)

            # è§†é¢‘ç±»å‹ï¼šæ£€æŸ¥ä¸‹è½½åœ°å€æ˜¯å¦ä¸ºç©º
            elif result["ä½œå“ç±»å‹"] == "è§†é¢‘":
                download_list = result.get("ä¸‹è½½åœ°å€", [])
                if not download_list or len(download_list) == 0:
                    logger.warning(f"ğŸŒ [Patch] XHS video has no download URLs, returning empty VideoParseResult")
                    return VideoParseResult(video=None, **k)
                else:
                    return VideoParseResult(video=download_list[0], **k)

            # å›¾æ–‡ç±»å‹ï¼šæ£€æŸ¥ä¸‹è½½åœ°å€æ˜¯å¦ä¸ºç©º
            elif result["ä½œå“ç±»å‹"] == "å›¾æ–‡":
                download_list = result.get("ä¸‹è½½åœ°å€", [])
                if not download_list:
                    logger.warning(f"ğŸŒ [Patch] XHS images have no download URLs, returning empty ImageParseResult")
                    return ImageParseResult(photo=[], **k)

                photos = []
                for i in download_list:
                    # Remove ?imageView2/format/png params - causes CDN 500 error
                    # Use base URL without params
                    img_url = i.split('?')[0] if '?' in i else i
                    ext = (await self.get_ext_by_url(img_url)) or "png"
                    photos.append(Image(img_url, ext))
                return ImageParseResult(photo=photos, **k)

            else:
                raise ParseError("ä¸æ”¯æŒçš„ç±»å‹")

        XhsParser.parse = patched_xhs_parse
        logger.info("âœ… XhsParser patched: handle empty download list")

        # Patch DouyinParser to use TikHub API for direct download
        from parsehub.parsers.parser import DouyinParser

        original_douyin_parse = DouyinParser.parse

        async def patched_douyin_parse(self, url: str):
            """Patched parse that uses TikHub as fallback when official API fails"""
            from parsehub.types import VideoParseResult
            from parsehub.types.error import ParseError

            # Try official parser first
            try:
                return await original_douyin_parse(self, url)
            except (ParseError, Exception) as e:
                logger.warning(f"âš ï¸ [Douyin] Official parser failed: {e}, trying TikHub...")

            # Fallback to TikHub
            tikhub_api_key = os.getenv("TIKHUB_API_KEY")
            if not tikhub_api_key:
                raise ParseError("å®˜æ–¹è§£æå¤±è´¥ä¸”æœªé…ç½®TikHub API")

            try:
                url = await self.get_raw_url(url)

                # Extract aweme_id from URL
                aweme_id_match = re.search(r'modal_id=(\d+)', url)
                if not aweme_id_match:
                    aweme_id_match = re.search(r'/video/(\d+)', url)

                if not aweme_id_match:
                    raise ParseError("æ— æ³•ä»URLæå–aweme_id")

                aweme_id = aweme_id_match.group(1)
                logger.info(f"ğŸ¬ [TikHub] Fetching Douyin video via TikHub: {aweme_id}")

                # Call TikHub Douyin API
                api_url = f"https://api.tikhub.io/api/v1/douyin/web/fetch_one_video?aweme_id={aweme_id}"
                headers = {"Authorization": f"Bearer {tikhub_api_key}"}

                with httpx.Client(timeout=30.0) as client:
                    response = client.get(api_url, headers=headers)

                if response.status_code != 200:
                    raise ParseError(f"TikHub APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")

                data = response.json()
                if data.get("code") != 200 or not data.get("data"):
                    raise ParseError(f"TikHub APIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")

                aweme_detail = data["data"]["aweme_detail"]
                video = aweme_detail.get("video", {})
                bit_rates = video.get("bit_rate", [])

                if not bit_rates:
                    raise ParseError("TikHubè¿”å›æ•°æ®ä¸­æ²¡æœ‰è§†é¢‘")

                # Use best quality (first item)
                best_video = bit_rates[0]
                play_addr = best_video.get("play_addr", {})
                url_list = play_addr.get("url_list", [])

                if not url_list:
                    raise ParseError("TikHubè¿”å›æ•°æ®ä¸­æ²¡æœ‰ä¸‹è½½URL")

                download_url = url_list[0]
                title = aweme_detail.get("desc", "")

                file_size_mb = play_addr.get("data_size", 0) / 1024 / 1024
                logger.info(f"âœ… [TikHub] Got Douyin video ({best_video.get('gear_name', 'unknown')}, {file_size_mb:.2f}MB)")

                return VideoParseResult(
                    raw_url=url,
                    title=title,
                    desc=title,
                    video=download_url,
                )

            except Exception as e:
                logger.error(f"âŒ [TikHub] Douyinè§£æå¤±è´¥: {e}")
                raise ParseError(f"TikHubè§£æå¤±è´¥: {e}")

        DouyinParser.parse = patched_douyin_parse
        logger.info("âœ… DouyinParser patched: use TikHub as fallback when official parser fails")

        # Patch DouyinParser to handle TikTok with TikHub API
        original_douyin_parse_api = DouyinParser.parse_api

        async def patched_douyin_parse_with_tiktok(self, url: str):
            """Enhanced parse that uses TikHub for TikTok videos"""
            from parsehub.types import VideoParseResult, ImageParseResult
            from parsehub.types.error import ParseError

            # Check if it's a TikTok URL
            is_tiktok = "tiktok.com" in url.lower()

            if not is_tiktok:
                # For Douyin, use the existing patched version
                return await patched_douyin_parse(self, url)

            # For TikTok, use TikHub API
            tikhub_api_key = os.getenv("TIKHUB_API_KEY")
            if not tikhub_api_key:
                logger.warning("âš ï¸ [TikTok] TIKHUB_API_KEY not configured, trying official parser...")
                try:
                    return await patched_douyin_parse(self, url)
                except Exception as e:
                    raise ParseError(f"TikTokè§£æå¤±è´¥ä¸”æœªé…ç½®TikHub API: {e}")

            try:
                url = await self.get_raw_url(url)
                logger.info(f"ğŸ¬ [TikHub] Parsing TikTok video: {url[:80]}...")

                # Extract video ID from TikTok URL
                video_id_match = re.search(r'/video/(\d+)', url)
                if not video_id_match:
                    raise ParseError("æ— æ³•ä»URLæå–TikTokè§†é¢‘ID")

                video_id = video_id_match.group(1)
                logger.info(f"ğŸ¬ [TikHub] Fetching TikTok video ID: {video_id}")

                # Call TikHub TikTok API (use app/v3 endpoint)
                api_url = f"https://api.tikhub.io/api/v1/tiktok/app/v3/fetch_one_video?aweme_id={video_id}"
                headers = {"Authorization": f"Bearer {tikhub_api_key}"}

                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(api_url, headers=headers)

                if response.status_code != 200:
                    raise ParseError(f"TikHub APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")

                data = response.json()
                if data.get("code") != 200:
                    raise ParseError(f"TikHub APIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")

                if not data.get("data"):
                    raise ParseError("TikHub APIè¿”å›ç©ºæ•°æ®")

                aweme_detail = data["data"].get("aweme_detail")
                if not aweme_detail:
                    raise ParseError("TikHubè¿”å›æ•°æ®ä¸­æ²¡æœ‰aweme_detail")

                desc = aweme_detail.get("desc", "")
                video = aweme_detail.get("video", {})

                # Check if it's an image post (photo carousel)
                image_post_info = aweme_detail.get("image_post_info")
                if image_post_info:
                    from parsehub.types import Image
                    images = image_post_info.get("images", [])
                    if images:
                        image_list = []
                        for img in images:
                            image_url_list = img.get("display_image", {}).get("url_list", [])
                            if image_url_list:
                                image_list.append(Image(image_url_list[0]))

                        logger.info(f"âœ… [TikHub] Got TikTok image post with {len(image_list)} images")
                        return ImageParseResult(
                            raw_url=url,
                            title=desc,
                            photo=image_list,
                        )

                # Handle video post - use best quality from bit_rate list
                bit_rates = video.get("bit_rate", [])
                if bit_rates:
                    # Sort by quality (highest first) and get best quality
                    bit_rates.sort(key=lambda x: x.get("bit_rate", 0), reverse=True)
                    best_video = bit_rates[0]
                    play_addr = best_video.get("play_addr", {})
                    url_list = play_addr.get("url_list", [])

                    if url_list:
                        download_url = url_list[0]
                        width = play_addr.get("width", 0)
                        height = play_addr.get("height", 0)
                        duration = play_addr.get("duration", 0) // 1000  # Convert ms to seconds

                        logger.info(f"âœ… [TikHub] Got TikTok video ({best_video.get('gear_name', 'unknown')}, {width}x{height}, {duration}s)")

                        from parsehub.types import Video
                        return VideoParseResult(
                            raw_url=url,
                            title=desc,
                            video=Video(
                                download_url,
                                width=width,
                                height=height,
                                duration=duration,
                            ),
                        )

                # Fallback: try play_addr directly
                play_addr = video.get("play_addr", {})
                url_list = play_addr.get("url_list", [])
                if url_list:
                    download_url = url_list[0]
                    duration = video.get("duration", 0) // 1000

                    logger.info(f"âœ… [TikHub] Got TikTok video (fallback URL)")

                    from parsehub.types import Video
                    return VideoParseResult(
                        raw_url=url,
                        title=desc,
                        video=Video(download_url, duration=duration),
                    )

                raise ParseError("TikHubè¿”å›æ•°æ®ä¸­æ²¡æœ‰å¯ç”¨çš„è§†é¢‘æˆ–å›¾ç‰‡")

            except ParseError:
                raise
            except Exception as e:
                logger.error(f"âŒ [TikHub] TikTokè§£æå¤±è´¥: {e}")
                # Try official parser as last resort
                try:
                    logger.info("ğŸ”„ [TikTok] Trying official parser as fallback...")
                    return await patched_douyin_parse(self, url)
                except Exception as fallback_error:
                    raise ParseError(f"TikHubå’Œå®˜æ–¹è§£æå™¨éƒ½å¤±è´¥: TikHub={e}, Official={fallback_error}")

        DouyinParser.parse = patched_douyin_parse_with_tiktok
        logger.info("âœ… DouyinParser patched: TikHub support for TikTok videos and images")

        return True

    except Exception as e:
        logger.error(f"âŒ ParseHub patch failed: {e}")
        return False
