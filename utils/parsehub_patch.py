"""
Monkey patch for ParseHub to fix issues:
1. YtParser format selector: Invalid format causes Facebook/YouTube videos to fail
2. YtParser cookie handling: YtParser doesn't pass cookies to yt-dlp
3. BiliAPI anti-crawler: BiliAPI doesn't set Referer headers for API calls
4. XhsParser empty download list: XhsParser crashes when download_list is empty
"""


def patch_parsehub_yt_dlp():
    """
    Patch ParseHub's YtParser to:
    1. Use correct format selector
    2. Pass cookies from ParseConfig to yt-dlp
    3. Patch BiliAPI to add Referer headers for anti-crawler
    4. Patch XhsParser to handle empty download list gracefully
    """
    try:
        import logging
        import os
        import tempfile
        logger = logging.getLogger(__name__)

        from parsehub.parsers.base.yt_dlp_parser import YtParser
        from parsehub.provider_api.bilibili import BiliAPI
        from parsehub.parsers.parser.xhs_ import XhsParser

        logger.info("ğŸ”§ Starting ParseHub patch...")

        def fixed_extract_info(self, url):
            """Fixed _extract_info that passes cookies to yt-dlp and stores TikHub download URL"""
            import re
            import httpx
            from yt_dlp import YoutubeDL

            params = self.params.copy()

            # Add proxy if configured
            if self.cfg.proxy:
                params["proxy"] = self.cfg.proxy

            # JavaScript runtimeé…ç½®ï¼š
            # yt-dlpé»˜è®¤æ”¯æŒdenoï¼Œä¼šè‡ªåŠ¨æ£€æµ‹PATHä¸­çš„deno
            # ä¸éœ€è¦æ‰‹åŠ¨é…ç½®js_runtimes (Dockerfileå·²å®‰è£…denoå¹¶æ·»åŠ åˆ°PATH)

            # Add headers (Referer/Origin) for anti-crawler
            # yt-dlpéœ€è¦è¿™äº›headersæ‰èƒ½ç»•è¿‡å„å¹³å°çš„åçˆ¬è™«æ£€æµ‹
            # é‡è¦ï¼šä¸è¦è¦†ç›–params["http_headers"]ï¼Œè€Œæ˜¯æ›´æ–°ç°æœ‰headers
            # å‚è€ƒ: yt_dlp/YoutubeDL.py:742 - params['http_headers'] = HTTPHeaderDict(std_headers, self.params.get('http_headers'))
            url_lower = url.lower()

            # è·å–ç°æœ‰headersï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ç©ºdict
            http_headers = params.get("http_headers", {})
            if not isinstance(http_headers, dict):
                http_headers = {}

            # ä¸è®¾ç½®User-Agentï¼Œè®©yt-dlpä½¿ç”¨random_user_agent()ï¼ˆæ›´å¥½çš„åçˆ¬è™«ï¼‰
            # å‚è€ƒ: yt_dlp/utils/networking.py:162 - 'User-Agent': random_user_agent()

            # æ ¹æ®å¹³å°æ·»åŠ Refererå’ŒOriginï¼ˆè¿™äº›æ˜¯å¿…éœ€çš„åçˆ¬è™«headersï¼‰
            if "youtube.com" in url_lower or "youtu.be" in url_lower:
                http_headers.update({
                    "Referer": "https://www.youtube.com/",
                    "Origin": "https://www.youtube.com"
                })
                logger.info(f"ğŸŒ [Patch] Added YouTube headers (Referer/Origin)")
            elif "bilibili.com" in url_lower or "b23.tv" in url_lower:
                http_headers.update({
                    "Referer": "https://www.bilibili.com/",
                    "Origin": "https://www.bilibili.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Bilibili headers (Referer/Origin)")
            elif "twitter.com" in url_lower or "x.com" in url_lower:
                http_headers.update({
                    "Referer": "https://twitter.com/",
                    "Origin": "https://twitter.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Twitter headers (Referer/Origin)")
            elif "instagram.com" in url_lower:
                http_headers.update({
                    "Referer": "https://www.instagram.com/",
                    "Origin": "https://www.instagram.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Instagram headers (Referer/Origin)")
            elif "kuaishou.com" in url_lower:
                http_headers.update({
                    "Referer": "https://www.kuaishou.com/",
                    "Origin": "https://www.kuaishou.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Kuaishou headers (Referer/Origin)")
            elif "facebook.com" in url_lower or "fb.watch" in url_lower:
                http_headers.update({
                    "Referer": "https://www.facebook.com/",
                    "Origin": "https://www.facebook.com"
                })
                logger.info(f"ğŸŒ [Patch] Added Facebook headers (Referer/Origin)")

            # æ›´æ–°paramsï¼ˆè€Œä¸æ˜¯è¦†ç›–ï¼‰
            params["http_headers"] = http_headers
            logger.info(f"ğŸ” [Patch] Final http_headers: {http_headers}")

            # Add cookies if configured (FIX: YtParser doesn't handle cookies)
            # å‚è€ƒ: yt_dlp/YoutubeDL.py:349 - cookiefile: File name or text stream from where cookies should be read
            temp_cookie_file = None

            # YouTubeç‰¹æ®Šå¤„ç†ï¼šä»ç¯å¢ƒå˜é‡è¯»å–cookieæ–‡ä»¶è·¯å¾„
            # ï¼ˆå› ä¸ºParseConfigä¼šæŠŠæ–‡ä»¶è·¯å¾„è§£ææˆdictï¼‰
            # ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > ParseConfig
            if ("youtube.com" in url.lower() or "youtu.be" in url.lower()) and "cookiefile" not in params:
                youtube_cookie_from_env = os.getenv("YOUTUBE_COOKIE")
                if youtube_cookie_from_env:
                    logger.info(f"ğŸª [Patch] YouTube cookie from env: {youtube_cookie_from_env}")
                    if os.path.exists(youtube_cookie_from_env):
                        params["cookiefile"] = youtube_cookie_from_env
                        logger.info(f"ğŸª [Patch] Using YouTube cookie file: {youtube_cookie_from_env}")
                    else:
                        logger.warning(f"âš ï¸ [Patch] YouTube cookie file not found: {youtube_cookie_from_env}")

            # å…¶ä»–å¹³å°cookieå¤„ç†ï¼ˆä»ParseConfigä¼ é€’ï¼‰
            # åªæœ‰åœ¨cookiefileè¿˜æ²¡è®¾ç½®æ—¶æ‰å¤„ç†
            if self.cfg.cookie and "cookiefile" not in params:
                logger.info(f"ğŸª [Patch] Received cookie type: {type(self.cfg.cookie)}, value preview: {str(self.cfg.cookie)[:100]}")
                # æ£€æŸ¥cookieç±»å‹ï¼šæ–‡ä»¶è·¯å¾„æˆ–å­—ç¬¦ä¸²
                if isinstance(self.cfg.cookie, str):
                    logger.info(f"ğŸª [Patch] Cookie is string, checking if file exists: {self.cfg.cookie}")
                    # åˆ¤æ–­æ˜¯æ–‡ä»¶è·¯å¾„è¿˜æ˜¯cookieå­—ç¬¦ä¸²
                    if os.path.exists(self.cfg.cookie):
                        logger.info(f"ğŸª [Patch] File exists! Setting cookiefile parameter")
                        # Netscapeæ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                        params["cookiefile"] = self.cfg.cookie
                        logger.info(f"ğŸª [Patch] Using cookie file: {self.cfg.cookie}")
                    else:
                        # Bilibili/Twitterç­‰cookieå­—ç¬¦ä¸²ï¼Œè§£æåå†™ä¸´æ—¶æ–‡ä»¶
                        logger.info(f"ğŸª [Patch] Parsing cookie string (len={len(self.cfg.cookie)})")

                        # è§£æcookieå­—ç¬¦ä¸²ä¸ºdict
                        cookie_dict = {}
                        for item in self.cfg.cookie.split(';'):
                            item = item.strip()
                            if '=' in item:
                                key, value = item.split('=', 1)
                                cookie_dict[key.strip()] = value.strip()

                        # æ ¹æ®URLåˆ¤æ–­domain
                        url_lower = url.lower()
                        if "bili" in url_lower:
                            domain = ".bilibili.com"
                        elif "twitter.com" in url_lower or "x.com" in url_lower:
                            domain = ".twitter.com"
                        elif "instagram.com" in url_lower:
                            domain = ".instagram.com"
                        elif "kuaishou.com" in url_lower:
                            domain = ".kuaishou.com"
                        else:
                            domain = ".example.com"

                        # å†™å…¥ä¸´æ—¶Netscapeæ ¼å¼æ–‡ä»¶
                        temp_cookie_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
                        temp_cookie_file.write("# Netscape HTTP Cookie File\n")
                        for key, value in cookie_dict.items():
                            temp_cookie_file.write(f"{domain}\tTRUE\t/\tFALSE\t0\t{key}\t{value}\n")
                        temp_cookie_file.close()

                        params["cookiefile"] = temp_cookie_file.name
                        logger.info(f"ğŸª [Patch] Created temp cookie file for {domain}")

            try:
                with YoutubeDL(params) as ydl:
                    result = ydl.extract_info(url, download=False)

                # æ¸…ç†ä¸´æ—¶cookieæ–‡ä»¶
                if temp_cookie_file and os.path.exists(temp_cookie_file.name):
                    os.unlink(temp_cookie_file.name)

                # For YouTube URLs, try to get TikHub direct download URL
                youtube_patterns = [
                    r'(?:https?://)?(?:www\.)?(?:youtube\.com|youtu\.be)/',
                    r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=',
                    r'(?:https?://)?youtu\.be/'
                ]
                is_youtube = any(re.search(pattern, url, re.IGNORECASE) for pattern in youtube_patterns)

                if is_youtube and result:
                    try:
                        # Extract video ID from URL
                        video_id_match = re.search(r'(?:v=|/)([0-9A-Za-z_-]{11})(?:[?&]|$)', url)
                        if video_id_match:
                            video_id = video_id_match.group(1)

                            # TikHub API key from config (via environment variable TIKHUB_API_KEY)
                            # Note: We use os.getenv here because self.cfg is ParseConfig, not BotConfig
                            # The API key should be set in environment variable TIKHUB_API_KEY
                            tikhub_api_key = os.getenv("TIKHUB_API_KEY")
                            if not tikhub_api_key:
                                logger.debug(f"âš ï¸ [TikHub] TIKHUB_API_KEY not set, skipping TikHub API")
                                return result

                            logger.info(f"ğŸ¬ [TikHub] Fetching direct download URL for YouTube video: {video_id}")

                            # Call TikHub API
                            api_url = f"https://api.tikhub.io/api/v1/youtube/web/get_video_info?video_id={video_id}"
                            headers = {"Authorization": f"Bearer {tikhub_api_key}"}

                            with httpx.Client(timeout=30.0) as client:
                                response = client.get(api_url, headers=headers)

                            if response.status_code == 200:
                                data = response.json()
                                if data.get("code") == 200 and data.get("data"):
                                    video_data = data["data"]
                                    videos = video_data.get("videos", {}).get("items", [])

                                    if videos:
                                        # Find best video with audio (itag=18 is 360p with audio)
                                        best_video = None
                                        for v in videos:
                                            if v.get("hasAudio"):
                                                best_video = v
                                                break

                                        if not best_video:
                                            # No video with audio, use first video
                                            best_video = videos[0]

                                        # Store TikHub direct URL in result
                                        result["_tikhub_url"] = best_video["url"]
                                        result["_tikhub_quality"] = best_video.get("quality", "unknown")
                                        logger.info(f"âœ… [TikHub] Got direct download URL ({best_video.get('quality', 'unknown')}, {best_video.get('sizeText', 'unknown')})")
                                    else:
                                        logger.warning(f"âš ï¸ [TikHub] No videos found in API response")
                                else:
                                    logger.warning(f"âš ï¸ [TikHub] API returned error: {data.get('message', 'Unknown error')}")
                            else:
                                logger.warning(f"âš ï¸ [TikHub] API request failed: HTTP {response.status_code}")

                    except Exception as e:
                        logger.warning(f"âš ï¸ [TikHub] Failed to fetch direct URL: {e}")

                return result
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶cookieæ–‡ä»¶
                if temp_cookie_file and os.path.exists(temp_cookie_file.name):
                    os.unlink(temp_cookie_file.name)
                error_msg = f"{type(e).__name__}: {str(e)}"
                raise RuntimeError(error_msg) from None

        # Apply YtParser patches
        # Note: Don't patch params property - it breaks subtitle configs and other settings
        # Only patch _extract_info method which handles js_runtimes internally
        YtParser._extract_info = fixed_extract_info
        logger.info("âœ… YtParser patched: js_runtimes + cookie handling + headers")

        # Patch BiliAPI to support cookies and add Referer headers
        # Problem: BiliAPI.__init__ doesn't accept cookie parameter
        # Solution: Patch __init__ to accept cookie, and patch get_video_info to use it
        original_bili_init = BiliAPI.__init__
        original_get_video_info = BiliAPI.get_video_info

        def patched_bili_init(self, proxy: str = None, cookie: dict = None):
            """Patched BiliAPI.__init__ to accept cookie parameter"""
            original_bili_init(self, proxy)
            # ä¿å­˜cookieä¾›APIè°ƒç”¨ä½¿ç”¨
            self.cookie = cookie
            # æ·»åŠ Refererå’ŒOrigin headers
            self.headers.update({
                "Referer": "https://www.bilibili.com/",
                "Origin": "https://www.bilibili.com"
            })
            if cookie:
                logger.info(f"ğŸŒ [Patch] BiliAPI initialized with cookie and anti-crawler headers")
            else:
                logger.info(f"ğŸŒ [Patch] BiliAPI initialized with anti-crawler headers (no cookie)")

        async def patched_get_video_info(self, url: str):
            """Patched get_video_info to use self.cookie"""
            bvid = self.get_bvid(url)
            # ä½¿ç”¨self.cookieè€Œä¸æ˜¯ç¡¬ç¼–ç None
            response = await self._get_client().get(
                "https://api.bilibili.com/x/web-interface/view/detail",
                params={"bvid": bvid},
                cookies=self.cookie  # ä¼ å…¥cookieï¼
            )
            return response.json()

        BiliAPI.__init__ = patched_bili_init
        BiliAPI.get_video_info = patched_get_video_info

        # Note: BiliParse.bili_api_parse creates BiliAPI without cookie
        # But since we patched BiliAPI.__init__ to accept cookie parameter,
        # we need to ensure cookie is passed. The easiest way is to patch
        # the BiliAPI creation call in BiliParse, but that's complex.
        #
        # Instead, we rely on the fact that ParseConfig.cookie is accessible
        # via self.cfg.cookie in BiliParse. We just need BiliParse to pass it.
        #
        # Since we can't easily modify the calling code, we make BiliAPI
        # read cookie from environment if not provided in __init__.

        # Update: Simplify - patch BiliAPI to read from environment variable
        original_bili_init_v2 = BiliAPI.__init__

        def patched_bili_init_v2(self, proxy: str = None, cookie: dict = None):
            """Enhanced BiliAPI.__init__ that reads cookie from env if not provided"""
            # å¦‚æœæ²¡ä¼ cookieï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
            if not cookie:
                import os
                cookie_str = os.getenv("BILIBILI_COOKIE")
                if cookie_str:
                    cookie = {}
                    for item in cookie_str.split(';'):
                        item = item.strip()
                        if '=' in item:
                            key, value = item.split('=', 1)
                            cookie[key.strip()] = value.strip()
                    logger.info(f"ğŸŒ [Patch] BiliAPI loaded cookie from environment")

            # è°ƒç”¨ä¹‹å‰patchçš„ç‰ˆæœ¬
            patched_bili_init(self, proxy, cookie)

        BiliAPI.__init__ = patched_bili_init_v2
        logger.info("âœ… BiliAPI patched: cookie support (from env) + anti-crawler headers")

        # Patch XhsParser to handle empty download list
        # Reference: parsehub/parsers/parser/xhs_.py - parse method line 15
        original_xhs_parse = XhsParser.parse

        async def patched_xhs_parse(self, url: str):
            """Patched XhsParser.parse to handle empty download list"""
            from parsehub.types import VideoParseResult, ImageParseResult, MultimediaParseResult, Video, Image
            from parsehub.parsers.parser.xhs_ import XHS, Log

            # è°ƒç”¨åŸå§‹é€»è¾‘è·å–æ•°æ®
            url = await self.get_raw_url(url)
            async with XHS(user_agent="", cookie="") as xhs:
                x_result = await xhs.extract(url, False, log=Log)

            from parsehub.types.error import ParseError
            if not x_result or not (result := x_result[0]):
                raise ParseError("å°çº¢ä¹¦è§£æå¤±è´¥")

            desc = self.hashtag_handler(result["ä½œå“æè¿°"])
            k = {"title": result["ä½œå“æ ‡é¢˜"], "desc": desc, "raw_url": url}

            # Livephotoå¤„ç†
            if all(result["åŠ¨å›¾åœ°å€"]):
                return MultimediaParseResult(media=[Video(i) for i in result["åŠ¨å›¾åœ°å€"]], **k)

            # è§†é¢‘ç±»å‹ï¼šæ£€æŸ¥ä¸‹è½½åœ°å€æ˜¯å¦ä¸ºç©º
            elif result["ä½œå“ç±»å‹"] == "è§†é¢‘":
                download_list = result.get("ä¸‹è½½åœ°å€", [])
                if not download_list or len(download_list) == 0:
                    logger.warning(f"ğŸŒ [Patch] XHS video has no download URLs, returning empty VideoParseResult")
                    return VideoParseResult(video=None, **k)
                else:
                    return VideoParseResult(video=download_list[0], **k)

            # å›¾æ–‡ç±»å‹ï¼šæ£€æŸ¥ä¸‹è½½åœ°å€æ˜¯å¦ä¸ºç©º
            elif result["ä½œå“ç±»å‹"] == "å›¾æ–‡":
                download_list = result.get("ä¸‹è½½åœ°å€", [])
                if not download_list:
                    logger.warning(f"ğŸŒ [Patch] XHS images have no download URLs, returning empty ImageParseResult")
                    return ImageParseResult(photo=[], **k)

                photos = []
                for i in download_list:
                    # Remove ?imageView2/format/png params - causes CDN 500 error
                    # Use base URL without params
                    img_url = i.split('?')[0] if '?' in i else i
                    ext = (await self.get_ext_by_url(img_url)) or "png"
                    photos.append(Image(img_url, ext))
                return ImageParseResult(photo=photos, **k)

            else:
                raise ParseError("ä¸æ”¯æŒçš„ç±»å‹")

        XhsParser.parse = patched_xhs_parse
        logger.info("âœ… XhsParser patched: handle empty download list")

        return True

    except Exception as e:
        logger.error(f"âŒ ParseHub patch failed: {e}")
        return False
