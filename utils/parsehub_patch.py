"""
Monkey patch for ParseHub to fix issues:
1. YtParser format selector: Invalid format causes Facebook/YouTube videos to fail
2. YtParser cookie handling: YtParser doesn't pass cookies to yt-dlp
3. BiliAPI anti-crawler: BiliAPI doesn't set Referer headers for API calls
"""

def patch_parsehub_yt_dlp():
    """
    Patch ParseHub's YtParser to:
    1. Use correct format selector
    2. Pass cookies from ParseConfig to yt-dlp
    3. Patch BiliAPI to add Referer headers for anti-crawler
    """
    try:
        import logging
        import os
        import tempfile
        logger = logging.getLogger(__name__)

        from parsehub.parsers.base.yt_dlp_parser import YtParser
        from parsehub.provider_api.bilibili import BiliAPI

        logger.info("ðŸ”§ Starting ParseHub patch...")

        @property
        def fixed_params(self) -> dict:
            """Fixed params with correct format selector"""
            params = {
                "format": "bestvideo[height<=1080]+bestaudio/best",  # Fixed format
                "quiet": True,
                "playlist_items": "1",
            }
            return params

        def fixed_extract_info(self, url):
            """Fixed _extract_info that passes cookies to yt-dlp"""
            from yt_dlp import YoutubeDL

            params = self.params.copy()

            # Add proxy if configured
            if self.cfg.proxy:
                params["proxy"] = self.cfg.proxy

            # Add headers (Referer/Origin) for anti-crawler
            # yt-dlpéœ€è¦è¿™äº›headersæ‰èƒ½ç»•è¿‡å„å¹³å°çš„åçˆ¬è™«æ£€æµ‹
            url_lower = url.lower()
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }

            if "youtube.com" in url_lower or "youtu.be" in url_lower:
                headers.update({
                    "Referer": "https://www.youtube.com/",
                    "Origin": "https://www.youtube.com"
                })
                logger.info(f"ðŸŒ [Patch] Added YouTube headers (Referer/Origin)")
            elif "bilibili.com" in url_lower or "b23.tv" in url_lower:
                headers.update({
                    "Referer": "https://www.bilibili.com/",
                    "Origin": "https://www.bilibili.com"
                })
                logger.info(f"ðŸŒ [Patch] Added Bilibili headers (Referer/Origin)")
            elif "twitter.com" in url_lower or "x.com" in url_lower:
                headers.update({
                    "Referer": "https://twitter.com/",
                    "Origin": "https://twitter.com"
                })
                logger.info(f"ðŸŒ [Patch] Added Twitter headers (Referer/Origin)")
            elif "instagram.com" in url_lower:
                headers.update({
                    "Referer": "https://www.instagram.com/",
                    "Origin": "https://www.instagram.com"
                })
                logger.info(f"ðŸŒ [Patch] Added Instagram headers (Referer/Origin)")
            elif "kuaishou.com" in url_lower:
                headers.update({
                    "Referer": "https://www.kuaishou.com/",
                    "Origin": "https://www.kuaishou.com"
                })
                logger.info(f"ðŸŒ [Patch] Added Kuaishou headers (Referer/Origin)")
            elif "facebook.com" in url_lower or "fb.watch" in url_lower:
                headers.update({
                    "Referer": "https://www.facebook.com/",
                    "Origin": "https://www.facebook.com"
                })
                logger.info(f"ðŸŒ [Patch] Added Facebook headers (Referer/Origin)")

            params["http_headers"] = headers

            # Add cookies if configured (FIX: YtParser doesn't handle cookies)
            temp_cookie_file = None

            # YouTubeç‰¹æ®Šå¤„ç†ï¼šä»ŽçŽ¯å¢ƒå˜é‡è¯»å–ï¼ˆå› ä¸ºParseConfigä¼šæŠŠæ–‡ä»¶è·¯å¾„è§£æžæˆdictï¼‰
            youtube_cookie_from_env = None
            if "youtube.com" in url.lower() or "youtu.be" in url.lower():
                youtube_cookie_from_env = os.getenv("YOUTUBE_COOKIE")
                if youtube_cookie_from_env:
                    logger.info(f"ðŸª [Patch] YouTube cookie from env: {youtube_cookie_from_env}")
                    if os.path.exists(youtube_cookie_from_env):
                        params["cookiefile"] = youtube_cookie_from_env
                        logger.info(f"ðŸª [Patch] Using YouTube cookie file: {youtube_cookie_from_env}")
                    else:
                        logger.warning(f"âš ï¸ [Patch] YouTube cookie file not found: {youtube_cookie_from_env}")

            # å…¶ä»–å¹³å°cookieå¤„ç†ï¼ˆä»ŽParseConfigä¼ é€’ï¼‰
            if self.cfg.cookie:
                logger.info(f"ðŸª [Patch] Received cookie type: {type(self.cfg.cookie)}, value preview: {str(self.cfg.cookie)[:100]}")
                # æ£€æŸ¥cookieç±»åž‹ï¼šæ–‡ä»¶è·¯å¾„æˆ–å­—ç¬¦ä¸²
                if isinstance(self.cfg.cookie, str):
                    logger.info(f"ðŸª [Patch] Cookie is string, checking if file exists: {self.cfg.cookie}")
                    # åˆ¤æ–­æ˜¯æ–‡ä»¶è·¯å¾„è¿˜æ˜¯cookieå­—ç¬¦ä¸²
                    if os.path.exists(self.cfg.cookie):
                        logger.info(f"ðŸª [Patch] File exists! Setting cookiefile parameter")
                        # Netscapeæ–‡ä»¶è·¯å¾„ï¼Œç›´æŽ¥ä½¿ç”¨
                        params["cookiefile"] = self.cfg.cookie
                        logger.info(f"ðŸª [Patch] Using cookie file: {self.cfg.cookie}")
                    else:
                        # Bilibili/Twitterç­‰cookieå­—ç¬¦ä¸²ï¼Œè§£æžåŽå†™ä¸´æ—¶æ–‡ä»¶
                        logger.info(f"ðŸª [Patch] Parsing cookie string (len={len(self.cfg.cookie)})")

                        # è§£æžcookieå­—ç¬¦ä¸²ä¸ºdict
                        cookie_dict = {}
                        for item in self.cfg.cookie.split(';'):
                            item = item.strip()
                            if '=' in item:
                                key, value = item.split('=', 1)
                                cookie_dict[key.strip()] = value.strip()

                        # æ ¹æ®URLåˆ¤æ–­domain
                        url_lower = url.lower()
                        if "bili" in url_lower:
                            domain = ".bilibili.com"
                        elif "twitter.com" in url_lower or "x.com" in url_lower:
                            domain = ".twitter.com"
                        elif "instagram.com" in url_lower:
                            domain = ".instagram.com"
                        elif "kuaishou.com" in url_lower:
                            domain = ".kuaishou.com"
                        else:
                            domain = ".example.com"

                        # å†™å…¥ä¸´æ—¶Netscapeæ ¼å¼æ–‡ä»¶
                        temp_cookie_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
                        temp_cookie_file.write("# Netscape HTTP Cookie File\n")
                        for key, value in cookie_dict.items():
                            temp_cookie_file.write(f"{domain}\tTRUE\t/\tFALSE\t0\t{key}\t{value}\n")
                        temp_cookie_file.close()

                        params["cookiefile"] = temp_cookie_file.name
                        logger.info(f"ðŸª [Patch] Created temp cookie file for {domain}")

            try:
                with YoutubeDL(params) as ydl:
                    result = ydl.extract_info(url, download=False)

                # æ¸…ç†ä¸´æ—¶cookieæ–‡ä»¶
                if temp_cookie_file and os.path.exists(temp_cookie_file.name):
                    os.unlink(temp_cookie_file.name)

                return result
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶cookieæ–‡ä»¶
                if temp_cookie_file and os.path.exists(temp_cookie_file.name):
                    os.unlink(temp_cookie_file.name)
                error_msg = f"{type(e).__name__}: {str(e)}"
                raise RuntimeError(error_msg) from None

        # Apply YtParser patches
        YtParser.params = fixed_params
        YtParser._extract_info = fixed_extract_info
        logger.info("âœ… YtParser patched: format selector + cookie handling + headers")

        # Patch BiliAPI to add Referer headers for anti-crawler
        # ä¸èƒ½åªpatch __init__ï¼Œå› ä¸º_get_clientå¯èƒ½å¤ç”¨æ—§client
        # éœ€è¦patch _get_clientæ–¹æ³•ï¼Œå¼ºåˆ¶ä½¿ç”¨å¸¦Refererçš„headers
        original_get_client = BiliAPI._get_client

        def patched_get_client(self):
            """Patched BiliAPI._get_client with anti-crawler headers"""
            # ç¡®ä¿headersåŒ…å«Refererå’ŒOrigin
            if "Referer" not in self.headers:
                self.headers.update({
                    "Referer": "https://www.bilibili.com/",
                    "Origin": "https://www.bilibili.com"
                })
                logger.info("ðŸŒ [Patch] BiliAPI headers updated with Referer/Origin")

            # å¦‚æžœclientå·²å­˜åœ¨ä¸”æœªå…³é—­ï¼Œå…ˆå…³é—­æ—§clientä»¥åº”ç”¨æ–°headers
            if self._client is not None and not getattr(self._client, "is_closed", False):
                import asyncio
                # åŒæ­¥ä¸Šä¸‹æ–‡ä¸­æ— æ³•è°ƒç”¨å¼‚æ­¥acloseï¼Œç›´æŽ¥é‡ç½®
                self._client = None

            # è°ƒç”¨åŽŸå§‹æ–¹æ³•åˆ›å»ºæ–°clientï¼ˆä¼šä½¿ç”¨æ›´æ–°åŽçš„self.headersï¼‰
            return original_get_client(self)

        BiliAPI._get_client = patched_get_client
        logger.info("âœ… BiliAPI patched: anti-crawler headers")

        return True

    except Exception as e:
        logger.error(f"âŒ ParseHub patch failed: {e}")
        return False
