"""
åçˆ¬è™«ä»£ç†æœåŠ¡å™¨
è‡ªåŠ¨ä¸ºæ‰€æœ‰è¯·æ±‚æ·»åŠ ä¼ªè£…headersï¼Œç»•è¿‡Bç«™/YouTubeç­‰å¹³å°çš„åçˆ¬è™«æ£€æµ‹

ä½¿ç”¨è¯´æ˜ï¼š
1. è¿™æ˜¯ä¸€ä¸ªHTTPä»£ç†æœåŠ¡å™¨ï¼Œç›‘å¬æœ¬åœ°ç«¯å£ï¼ˆé»˜è®¤8765ï¼‰
2. è‡ªåŠ¨ä¸ºæ‰€æœ‰é€šè¿‡å®ƒçš„HTTPè¯·æ±‚æ·»åŠ ä¼ªè£…çš„æµè§ˆå™¨headers
3. æ”¯æŒå¹³å°ç‰¹å®šçš„Referer/Originè®¾ç½®
4. ä¿ç•™åŸå§‹Cookieå’ŒAuthorization headers
"""

import asyncio
import logging
from aiohttp import web
import aiohttp
from urllib.parse import urlparse

logger = logging.getLogger(__name__)


class AntiCrawlerProxy:
    """åçˆ¬è™«ä»£ç†æœåŠ¡å™¨ï¼Œè‡ªåŠ¨æ³¨å…¥headers"""

    def __init__(self, host: str = "127.0.0.1", port: int = 8765):
        self.host = host
        self.port = port
        self.app = web.Application()
        # æ³¨å†Œè·¯ç”± - å¤„ç†æ‰€æœ‰HTTPæ–¹æ³•
        self.app.router.add_route('*', '/{tail:.*}', self.handle_proxy)

        # ä¼ªè£…headers - æ¨¡æ‹ŸçœŸå®Chromeæµè§ˆå™¨
        self.fake_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
        }

        # å¹³å°ç‰¹å®šheadersï¼ˆæ·»åŠ Refererå’ŒOriginæ¥ä¼ªè£…æ¥æºï¼‰
        self.platform_headers = {
            'bilibili.com': {
                'Referer': 'https://www.bilibili.com/',
                'Origin': 'https://www.bilibili.com',
            },
            'b23.tv': {
                'Referer': 'https://www.bilibili.com/',
            },
            'youtube.com': {
                'Referer': 'https://www.youtube.com/',
                'Origin': 'https://www.youtube.com',
            },
            'youtu.be': {
                'Referer': 'https://www.youtube.com/',
            },
            'twitter.com': {
                'Referer': 'https://twitter.com/',
            },
            'x.com': {
                'Referer': 'https://x.com/',
            },
        }

    async def handle_proxy(self, request: web.Request):
        """
        å¤„ç†ä»£ç†è¯·æ±‚

        HTTPä»£ç†çš„å·¥ä½œæ–¹å¼ï¼š
        1. å®¢æˆ·ç«¯å‘é€è¯·æ±‚ï¼ŒURLæ ¼å¼ä¸ºï¼šhttp://proxy:port/real_url
        2. ä»£ç†è§£æreal_urlï¼Œæ·»åŠ ä¼ªè£…headers
        3. ä»£ç†è½¬å‘è¯·æ±‚åˆ°çœŸå®æœåŠ¡å™¨
        4. ä»£ç†è¿”å›å“åº”ç»™å®¢æˆ·ç«¯
        """
        try:
            # è·å–çœŸå®ç›®æ ‡URLï¼ˆä»è¯·æ±‚è·¯å¾„ä¸­æå–ï¼‰
            # ä¾‹å¦‚ï¼šè¯·æ±‚ http://127.0.0.1:8765/https://www.bilibili.com/video/xxx
            # æå–å‡ºï¼šhttps://www.bilibili.com/video/xxx
            path = request.match_info.get('tail', '')

            # å¦‚æœpathæ˜¯å®Œæ•´URLï¼ˆä»¥httpå¼€å¤´ï¼‰
            if path.startswith('http://') or path.startswith('https://'):
                target_url = path
            else:
                # å¦åˆ™ï¼Œä»X-Target-URL headerä¸­è·å–ï¼ˆyt-dlpç­‰å·¥å…·å¯èƒ½ä¼šè¿™æ ·ä¼ é€’ï¼‰
                target_url = request.headers.get('X-Target-URL')
                if not target_url:
                    # æœ€åå°è¯•ï¼šä½¿ç”¨Host headeræ„å»ºURL
                    host = request.headers.get('Host', request.url.host)
                    scheme = 'https' if request.url.scheme == 'https' else 'http'
                    target_url = f"{scheme}://{host}{request.path}"

            logger.info(f"ğŸ”„ ä»£ç†è¯·æ±‚: {request.method} {target_url}")

            # è§£æç›®æ ‡URLä»¥è·å–åŸŸå
            parsed = urlparse(target_url)
            target_domain = parsed.netloc

            # æ„å»ºheaders - ä»ä¼ªè£…headerså¼€å§‹
            headers = dict(self.fake_headers)

            # æ·»åŠ Host headerï¼ˆå¿…é¡»ï¼‰
            headers['Host'] = target_domain

            # æ·»åŠ å¹³å°ç‰¹å®šheaders
            for domain, platform_headers in self.platform_headers.items():
                if domain in target_domain:
                    headers.update(platform_headers)
                    logger.debug(f"ğŸ­ ä¸º {domain} æ·»åŠ å¹³å°ç‰¹å®šheaders")
                    break

            # ä¿ç•™åŸå§‹è¯·æ±‚ä¸­çš„é‡è¦headers
            preserve_headers = [
                'Cookie', 'Authorization', 'Range',
                'If-None-Match', 'If-Modified-Since'
            ]
            for header in preserve_headers:
                if header in request.headers:
                    headers[header] = request.headers[header]
                    if header == 'Cookie':
                        logger.debug(f"ğŸª ä¿ç•™Cookie: {request.headers[header][:50]}...")

            # ç§»é™¤å¯èƒ½æš´éœ²ä»£ç†èº«ä»½çš„headers
            headers.pop('X-Forwarded-For', None)
            headers.pop('Via', None)
            headers.pop('Proxy-Connection', None)
            headers.pop('X-Target-URL', None)

            # è¯»å–è¯·æ±‚ä½“ï¼ˆå¦‚æœæœ‰ï¼‰
            body = await request.read() if request.can_read_body and request.content_length else None

            # å‘é€è¯·æ±‚åˆ°ç›®æ ‡æœåŠ¡å™¨
            timeout = aiohttp.ClientTimeout(total=60, connect=10)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.request(
                    method=request.method,
                    url=target_url,
                    headers=headers,
                    data=body,
                    allow_redirects=True,
                    ssl=False  # è·³è¿‡SSLéªŒè¯ï¼ˆæœ‰äº›å¹³å°è¯ä¹¦å¯èƒ½æœ‰é—®é¢˜ï¼‰
                ) as resp:
                    # è¯»å–å“åº”ä½“
                    response_body = await resp.read()

                    # æ„å»ºå“åº”headers
                    response_headers = {}
                    for name, value in resp.headers.items():
                        # æ’é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„headers
                        if name.lower() not in ['transfer-encoding', 'content-encoding']:
                            response_headers[name] = value

                    logger.info(f"âœ… ä»£ç†å“åº”: {resp.status} ({len(response_body)} bytes)")

                    return web.Response(
                        body=response_body,
                        status=resp.status,
                        headers=response_headers
                    )

        except Exception as e:
            logger.error(f"âŒ ä»£ç†è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
            return web.Response(
                text=f"Proxy Error: {str(e)}",
                status=502
            )

    async def start(self):
        """å¯åŠ¨ä»£ç†æœåŠ¡å™¨"""
        runner = web.AppRunner(self.app)
        await runner.setup()
        site = web.TCPSite(runner, self.host, self.port)
        await site.start()
        logger.info(f"ğŸš€ åçˆ¬è™«ä»£ç†æœåŠ¡å™¨å·²å¯åŠ¨: http://{self.host}:{self.port}")
        logger.info(f"   ä½¿ç”¨æ–¹æ³•ï¼šåœ¨.envä¸­é…ç½®")
        logger.info(f"   PARSER_PROXY=http://{self.host}:{self.port}")
        logger.info(f"   DOWNLOADER_PROXY=http://{self.host}:{self.port}")

    def run(self):
        """è¿è¡Œä»£ç†æœåŠ¡å™¨ï¼ˆé˜»å¡æ¨¡å¼ï¼‰"""
        asyncio.run(self._run_async())

    async def _run_async(self):
        """å¼‚æ­¥è¿è¡Œä»£ç†æœåŠ¡å™¨"""
        await self.start()
        # ä¿æŒè¿è¡Œ
        await asyncio.Event().wait()


# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œå¯åŠ¨ä»£ç†æœåŠ¡å™¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¯åŠ¨ç‹¬ç«‹ä»£ç†æœåŠ¡å™¨")
    logger.info("   æŒ‰ Ctrl+C åœæ­¢")

    proxy = AntiCrawlerProxy(host="127.0.0.1", port=8765)

    try:
        proxy.run()
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ä»£ç†æœåŠ¡å™¨å·²åœæ­¢")
