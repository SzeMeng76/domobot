"""
ParseHub é€‚é…å™¨
å°† ParseHub åº“é€‚é…åˆ° python-telegram-bot æ¡†æ¶
æ•´åˆæ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼šå›¾åºŠä¸Šä¼ ã€AIæ€»ç»“ã€Telegraphå‘å¸ƒã€è§†é¢‘åˆ†å‰²ã€è½¬å½•ç­‰
"""

import asyncio
import hashlib
import logging
import tempfile
import time
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any

# Apply monkey patch to fix Facebook/YouTube parsing (ParseHub 1.5.10 format bug)
from utils.parsehub_patch import patch_parsehub_yt_dlp
patch_parsehub_yt_dlp()

from parsehub import ParseHub
from parsehub.config import DownloadConfig, ParseConfig, GlobalConfig
from parsehub.types import ParseResult, VideoParseResult, ImageParseResult, MultimediaParseResult

logger = logging.getLogger(__name__)


class ParseHubAdapter:
    """ParseHub é€‚é…å™¨ç±»"""

    def __init__(self, cache_manager=None, user_manager=None, config=None):
        """
        åˆå§‹åŒ–é€‚é…å™¨

        Args:
            cache_manager: Redis ç¼“å­˜ç®¡ç†å™¨
            user_manager: MySQL ç”¨æˆ·ç®¡ç†å™¨
            config: Boté…ç½®å¯¹è±¡
        """
        self.cache_manager = cache_manager
        self.user_manager = user_manager
        self.config = config
        self.parsehub = ParseHub()
        self.temp_dir = Path(tempfile.gettempdir()) / "domobot_parse"
        self.temp_dir.mkdir(exist_ok=True)

        # é…ç½® ParseHub GlobalConfig
        if config:
            if config.douyin_api:
                GlobalConfig.douyin_api = config.douyin_api
                logger.info(f"âœ… é…ç½®æŠ–éŸ³API: {config.douyin_api}")
            GlobalConfig.duration_limit = 0  # ä¸é™åˆ¶è§†é¢‘æ—¶é•¿

            # é…ç½®ä¼ªè£…User-Agentï¼ˆç»•è¿‡åçˆ¬è™«ï¼‰
            GlobalConfig.ua = (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            )
            logger.info(f"âœ… é…ç½®ä¼ªè£…User-Agent: Chrome/131.0.0.0")

    async def get_supported_platforms(self) -> List[str]:
        """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
        try:
            platforms = self.parsehub.get_supported_platforms()
            return platforms
        except Exception as e:
            logger.error(f"è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def check_url_supported(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ”¯æŒçš„å¹³å°URL

        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬

        Returns:
            bool: æ˜¯å¦åŒ…å«æ”¯æŒçš„URL
        """
        try:
            # æå– URL
            url = await self._extract_url(text)
            if not url:
                return False

            # æ£€æŸ¥æ˜¯å¦æ”¯æŒ
            parser = self.parsehub.select_parser(url)
            return parser is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥URLæ”¯æŒå¤±è´¥: {e}")
            return False

    async def parse_url(
        self,
        text: str,
        user_id: int,
        group_id: Optional[int] = None,
        proxy: Optional[str] = None
    ) -> Tuple[Optional[Any], Optional[str], float, Optional[str]]:
        """
        è§£æURLå¹¶ä¸‹è½½åª’ä½“

        Args:
            text: åŒ…å«URLçš„æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„IDï¼ˆå¯é€‰ï¼‰
            proxy: ä»£ç†åœ°å€ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ä»£ç†ï¼‰

        Returns:
            (DownloadResult, platform_name, parse_time, error_msg): ä¸‹è½½ç»“æœã€å¹³å°åç§°ã€è§£æè€—æ—¶ã€é”™è¯¯ä¿¡æ¯
        """
        start_time = time.time()

        try:
            # æå– URL
            url = await self._extract_url(text)
            if not url:
                return None, None, 0, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„URL"

            # æ³¨æ„ï¼šDownloadResultåŒ…å«æ–‡ä»¶å¯¹è±¡ï¼Œä¸èƒ½åºåˆ—åŒ–åˆ°Redisç¼“å­˜
            # æ¯æ¬¡éƒ½éœ€è¦é‡æ–°è§£æå’Œä¸‹è½½
            # cache_key = self._get_cache_key(url)

            # é€‰æ‹©è§£æå™¨
            parser = self.parsehub.select_parser(url)
            if not parser:
                logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {url}")
                return None, None, 0, "ä¸æ”¯æŒçš„å¹³å°"

            # è·å–å¹³å°ID
            platform_id = getattr(parser, '__platform_id__', '')
            platform_name = platform_id or "unknown"

            # é…ç½®ä»£ç†ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„proxyï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„proxyï¼‰
            # æ³¨æ„ï¼šPARSER_PROXY å·²å¼ƒç”¨ï¼Œæ”¹ç”¨ TIKTOK_REDIRECT_PROXYï¼ˆä»…ç”¨äºTikTokçŸ­é“¾æ¥é‡å®šå‘ï¼‰
            parser_proxy = proxy  # ä¸å†ä½¿ç”¨å…¨å±€ parser_proxy é…ç½®
            downloader_proxy = proxy or (self.config.downloader_proxy if self.config else None)

            # æ ¹æ®å¹³å°é€‰æ‹© Cookieï¼ˆParseConfigä¼šè‡ªåŠ¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºdictï¼‰
            # æ”¯æŒï¼šTwitter, Instagram, Bilibili, Kuaishou, YouTube (é€šè¿‡patch), Tieba
            # ä¸æ”¯æŒï¼šFacebook (åŸºäºyt-dlpï¼ŒParseHubåº“é™åˆ¶)
            platform_cookie = None
            if self.config:
                logger.info(f"ğŸ” å¹³å°: {platform_id}")
                if platform_id == 'twitter' and self.config.twitter_cookie:
                    platform_cookie = self.config.twitter_cookie
                    logger.info(f"âœ… ä½¿ç”¨Twitter cookie: {platform_cookie[:50]}...")
                elif platform_id == 'instagram' and self.config.instagram_cookie:
                    platform_cookie = self.config.instagram_cookie
                    logger.info(f"âœ… ä½¿ç”¨Instagram cookie")
                elif platform_id == 'bilibili' and self.config.bilibili_cookie:
                    platform_cookie = self.config.bilibili_cookie
                    logger.info(f"âœ… ä½¿ç”¨Bilibili cookie: {platform_cookie[:50]}...")
                elif platform_id == 'kuaishou' and self.config.kuaishou_cookie:
                    platform_cookie = self.config.kuaishou_cookie
                    logger.info(f"âœ… ä½¿ç”¨Kuaishou cookie")
                elif platform_id == 'tieba' and self.config.tieba_cookie:
                    platform_cookie = self.config.tieba_cookie
                    logger.info(f"âœ… ä½¿ç”¨Tieba cookie (ç»•è¿‡å®‰å…¨éªŒè¯): {platform_cookie[:50]}...")
                elif platform_id == 'youtube' and self.config.youtube_cookie:
                    # YouTube cookieæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œä¸èƒ½ä¼ ç»™ParseConfigï¼ˆä¼šè¢«è§£ææˆdictï¼‰
                    # ç›´æ¥åœ¨parsehub_patch.pyä¸­é€šè¿‡ç¯å¢ƒå˜é‡ YOUTUBE_COOKIE è¯»å–
                    platform_cookie = None
                    logger.info(f"âœ… YouTube cookieé…ç½®å­˜åœ¨ï¼Œé€šè¿‡patchä»ç¯å¢ƒå˜é‡è¯»å–")
                else:
                    logger.info(f"âš ï¸ å¹³å° {platform_id} æœªé…ç½®cookieæˆ–ä¸åŒ¹é…")

            # é…ç½®å¹³å°ç‰¹å®šçš„headersï¼ˆç”¨äºç»•è¿‡åçˆ¬è™«ï¼‰
            download_headers = {
                'User-Agent': GlobalConfig.ua,
            }

            # æ ¹æ®å¹³å°æ·»åŠ  Referer å’Œ Originï¼ˆç±»ä¼¼å†…ç½®ä»£ç†çš„ç­–ç•¥ï¼‰
            if platform_id == 'bilibili':
                download_headers.update({
                    'Referer': 'https://www.bilibili.com/',
                    'Origin': 'https://www.bilibili.com',
                })
                logger.info(f"âœ… é…ç½®Bilibili headers: Referer + Origin")
            elif platform_id == 'youtube':
                download_headers.update({
                    'Referer': 'https://www.youtube.com/',
                    'Origin': 'https://www.youtube.com',
                })
                logger.info(f"âœ… é…ç½®YouTube headers: Referer + Origin")
            elif platform_id == 'twitter':
                download_headers.update({
                    'Referer': 'https://twitter.com/',
                    'Origin': 'https://twitter.com',
                })
                logger.info(f"âœ… é…ç½®Twitter headers: Referer + Origin")
            elif platform_id == 'tiktok':
                download_headers.update({
                    'Referer': 'https://www.tiktok.com/',
                    'Origin': 'https://www.tiktok.com',
                })
                logger.info(f"âœ… é…ç½®TikTok headers: Referer + Origin")
            elif platform_id == 'xiaohongshu':
                download_headers.update({
                    'Referer': 'https://www.xiaohongshu.com/',
                    'Origin': 'https://www.xiaohongshu.com',
                })
                logger.info(f"âœ… é…ç½®å°çº¢ä¹¦ headers: Referer + Origin")

            # åˆ›å»ºé…ç½®ï¼ˆé‡è¦ï¼šæ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„ParseHubå®ä¾‹ï¼Œä¼ å…¥é…ç½®ï¼‰
            parse_config = ParseConfig(proxy=parser_proxy, cookie=platform_cookie)
            if platform_cookie:
                cookie_preview = str(platform_cookie)[:100] if not isinstance(platform_cookie, dict) else f"dict with {len(platform_cookie)} keys"
                logger.info(f"ğŸª ä¼ é€’ç»™ParseConfigçš„cookie - ç±»å‹: {type(platform_cookie)}, é¢„è§ˆ: {cookie_preview}")
            else:
                logger.info(f"âš ï¸ æœªä¼ é€’cookieç»™ParseConfig")
            download_config = DownloadConfig(
                proxy=downloader_proxy,
                save_dir=self.temp_dir,
                headers=download_headers  # ä¼ å…¥å¹³å°ç‰¹å®šheaders
            )

            # åˆ›å»ºæ–°çš„ParseHubå®ä¾‹å¹¶ä¼ å…¥é…ç½®
            parsehub = ParseHub(config=parse_config)
            result = await parsehub.parse(url)

            if not result:
                return None, None, 0, "è§£æå¤±è´¥ï¼Œæœªè¿”å›ç»“æœ"

            # ä¸‹è½½åª’ä½“
            try:
                download_result = await result.download(config=download_config)
            except Exception as download_error:
                # ä¸‹è½½å¤±è´¥ï¼ˆä¾‹å¦‚å°çº¢ä¹¦CDN 500é”™è¯¯ï¼‰ï¼Œä½†è§£ææˆåŠŸ
                # è¿”å›è§£æç»“æœä½†æ²¡æœ‰ä¸‹è½½çš„åª’ä½“æ–‡ä»¶
                logger.warning(f"åª’ä½“ä¸‹è½½å¤±è´¥ä½†è§£ææˆåŠŸ: {download_error}")
                # åˆ›å»ºä¸€ä¸ªç©ºçš„DownloadResultï¼ˆåªåŒ…å«è§£æä¿¡æ¯ï¼Œæ²¡æœ‰å®é™…æ–‡ä»¶ï¼‰
                from parsehub.types import DownloadResult
                download_result = DownloadResult(parse_result=result, media=None)

            parse_time = time.time() - start_time

            # æ³¨æ„ï¼šDownloadResultä¸èƒ½åºåˆ—åŒ–ï¼Œä¸ç¼“å­˜åˆ°Redis
            # ParseHubè‡ªå·±æœ‰å†…å­˜ç¼“å­˜æœºåˆ¶å¤„ç†é‡å¤URL

            # è®°å½•ç»Ÿè®¡
            await self._record_stats(user_id, group_id, platform_name, url, True, parse_time * 1000)

            return download_result, platform_name, parse_time, None

        except Exception as e:
            parse_time = time.time() - start_time
            error_msg = self._format_error_message(e)
            logger.error(f"è§£æURLå¤±è´¥: {error_msg}", exc_info=True)

            # è®°å½•å¤±è´¥ç»Ÿè®¡
            await self._record_stats(
                user_id,
                group_id,
                "unknown",
                text,
                False,
                parse_time * 1000,
                error_msg
            )

            return None, None, parse_time, error_msg

    def _format_error_message(self, error: Exception) -> str:
        """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ï¼Œä½¿å…¶å¯¹ç”¨æˆ·æ›´å‹å¥½"""
        error_str = str(error)

        # Bilibiliç›¸å…³é”™è¯¯
        if "412" in error_str or "Precondition Failed" in error_str:
            return "Bilibilié£æ§é™åˆ¶ï¼ˆ412é”™è¯¯ï¼‰ï¼Œè¯·æ£€æŸ¥cookieé…ç½®æˆ–ç¨åé‡è¯•"
        if "Bilibiliè§£æå¤±è´¥" in error_str:
            return "Bilibiliè§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯é“¾æ¥å¤±æ•ˆæˆ–éœ€è¦ç™»å½•"

        # YouTubeç›¸å…³é”™è¯¯
        if "Sign in to confirm" in error_str or "not a bot" in error_str:
            return "YouTubeéœ€è¦ç™»å½•éªŒè¯ï¼Œè¯·æ£€æŸ¥cookieé…ç½®"
        if "Requested format is not available" in error_str:
            return "YouTubeè§†é¢‘æ ¼å¼ä¸å¯ç”¨ï¼Œå¯èƒ½éœ€è¦å®‰è£…Node.jsæˆ–è§†é¢‘å·²è¢«åˆ é™¤"
        if "No supported JavaScript runtime" in error_str:
            return "ç¼ºå°‘JavaScriptè¿è¡Œç¯å¢ƒï¼ŒYouTubeè§£æå¯èƒ½å—é™"

        # Twitter/Xç›¸å…³é”™è¯¯
        if "twitter" in error_str.lower() or "x.com" in error_str.lower():
            return f"Twitter/Xè§£æå¤±è´¥ï¼š{error_str}"

        # é€šç”¨ç½‘ç»œé”™è¯¯
        if "timeout" in error_str.lower():
            return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        if "connection" in error_str.lower():
            return "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®"
        if "404" in error_str:
            return "å†…å®¹ä¸å­˜åœ¨ï¼ˆ404ï¼‰ï¼Œé“¾æ¥å¯èƒ½å·²å¤±æ•ˆ"
        if "403" in error_str:
            return "è®¿é—®è¢«æ‹’ç»ï¼ˆ403ï¼‰ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–cookie"
        if "500" in error_str or "502" in error_str or "503" in error_str:
            return "æœåŠ¡å™¨é”™è¯¯ï¼Œå¹³å°æœåŠ¡å¯èƒ½æš‚æ—¶ä¸å¯ç”¨"

        # Cookieç›¸å…³é”™è¯¯
        if "cookie" in error_str.lower():
            return f"Cookieé…ç½®é—®é¢˜ï¼š{error_str}"

        # ä¸‹è½½å¤±è´¥
        if "download" in error_str.lower():
            return f"ä¸‹è½½å¤±è´¥ï¼š{error_str}"

        # å…¶ä»–é”™è¯¯ï¼Œè¿”å›åŸå§‹é”™è¯¯ä¿¡æ¯ï¼ˆæˆªæ–­è¿‡é•¿çš„ä¿¡æ¯ï¼‰
        if len(error_str) > 150:
            return error_str[:150] + "..."
        return error_str

    async def format_result(self, download_result, platform: str) -> dict:
        """
        æ ¼å¼åŒ–ä¸‹è½½ç»“æœ

        Args:
            download_result: DownloadResult ä¸‹è½½ç»“æœ
            platform: å¹³å°åç§°

        Returns:
            dict: æ ¼å¼åŒ–åçš„ç»“æœ
        """
        try:
            # download_result.pr æ˜¯åŸå§‹çš„ ParseResult
            pr = download_result.pr
            formatted = {
                "title": pr.title or "",  # ä¿æŒåŸå§‹å€¼ï¼Œä¸è¦åœ¨è¿™é‡Œæ·»åŠ "æ— æ ‡é¢˜"
                "desc": pr.desc or "",
                "platform": platform,
                "url": pr.raw_url,
                "media_type": self._get_media_type(pr),
                "media_count": 0,
                "media_paths": []
            }

            # è·å–åª’ä½“è·¯å¾„
            media = download_result.media
            if isinstance(media, list):
                formatted["media_count"] = len(media)
                formatted["media_paths"] = [str(m.path) for m in media if m.exists()]
            elif media and media.exists():
                formatted["media_count"] = 1
                formatted["media_paths"] = [str(media.path)]

            return formatted

        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–ç»“æœå¤±è´¥: {e}")
            return {}

    def _get_media_type(self, result: ParseResult) -> str:
        """è·å–åª’ä½“ç±»å‹"""
        if isinstance(result, VideoParseResult):
            return "video"
        elif isinstance(result, ImageParseResult):
            return "image"
        elif isinstance(result, MultimediaParseResult):
            return "multimedia"
        else:
            return "unknown"

    async def _extract_url(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–URL"""
        import re
        import httpx

        try:
            # 1. å…ˆä»æ–‡æœ¬ä¸­æå–URL
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
            match = re.search(url_pattern, text)
            if not match:
                return None

            url = match.group(0)
            logger.debug(f"æå–åŸå§‹URL: {url}")

            # 2. æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­é“¾æ¥ï¼ˆéœ€è¦é‡å®šå‘ï¼‰
            short_domains = ['b23.tv', 't.co', 'bit.ly', 'youtu.be', 'tiktok.com/t/', 'vt.tiktok.com', 'vm.tiktok.com', 'douyin.com/']
            is_short = any(domain in url for domain in short_domains)

            if is_short:
                # è·Ÿéšé‡å®šå‘è·å–çœŸå®URL
                try:
                    # æ·»åŠ  User-Agent é¿å…è¢« TikTok é‡å®šå‘åˆ° notfound é¡µé¢
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
                    }
                    # ä½¿ç”¨ä»£ç†é¿å…åœ°åŒºé™åˆ¶ï¼ˆä»…ç”¨äºTikTokçŸ­é“¾æ¥é‡å®šå‘ï¼‰
                    import os
                    # æ£€æŸ¥æ˜¯å¦æ˜¯TikTokçŸ­é“¾æ¥
                    is_tiktok_short = any(domain in url for domain in ['vt.tiktok.com', 'vm.tiktok.com'])
                    proxy = os.getenv('TIKTOK_REDIRECT_PROXY') if is_tiktok_short else None
                    if proxy:
                        logger.info(f"âœ… [TikTokçŸ­é“¾æ¥] ä½¿ç”¨ä»£ç†é‡å®šå‘: {proxy[:30]}...")
                    async with httpx.AsyncClient(follow_redirects=True, timeout=10, headers=headers, proxy=proxy) as client:
                        response = await client.head(url)
                        final_url = str(response.url)
                        logger.info(f"çŸ­é“¾æ¥é‡å®šå‘: {url} -> {final_url}")

                        # æ£€æŸ¥æ˜¯å¦é‡å®šå‘åˆ° notfound é¡µé¢ï¼ˆåœ°åŒºé™åˆ¶ï¼‰
                        if '/notfound' in final_url.lower():
                            logger.error(f"è§†é¢‘ä¸å¯ç”¨ï¼ˆå¯èƒ½åœ°åŒºé™åˆ¶ï¼‰: {final_url}")
                            return None

                        url = final_url
                except Exception as e:
                    logger.warning(f"é‡å®šå‘å¤±è´¥ï¼Œä½¿ç”¨åŸURL: {e}")

            # 3. éªŒè¯URLæ˜¯å¦è¢«æ”¯æŒ
            parser = self.parsehub.select_parser(url)
            if not parser:
                logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {url}")
                return None

            logger.debug(f"è¯†åˆ«å¹³å°: {parser.__platform__}")
            return url

        except Exception as e:
            logger.error(f"æå–URLå¤±è´¥: {text}, é”™è¯¯: {e}", exc_info=True)
            return None

    def _get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return f"parse:result:{url_hash}"

    async def _record_stats(
        self,
        user_id: int,
        group_id: Optional[int],
        platform: str,
        url: str,
        success: bool,
        parse_time_ms: float,
        error_message: Optional[str] = None
    ):
        """è®°å½•è§£æç»Ÿè®¡"""
        if not self.user_manager:
            return

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        INSERT INTO social_parser_stats
                        (user_id, group_id, platform, url, parse_success, parse_time_ms, error_message)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """,
                        (user_id, group_id, platform, url, success, int(parse_time_ms), error_message)
                    )
                    await conn.commit()
        except Exception as e:
            logger.error(f"è®°å½•è§£æç»Ÿè®¡å¤±è´¥: {e}")

    async def cleanup_temp_files(self, older_than_hours: int = 24):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            cutoff_time = time.time() - (older_than_hours * 3600)

            for item in self.temp_dir.iterdir():
                if item.is_dir() and item.stat().st_mtime < cutoff_time:
                    shutil.rmtree(item, ignore_errors=True)
                    logger.info(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {item}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    async def is_auto_parse_enabled(self, group_id: int) -> bool:
        """
        æ£€æŸ¥ç¾¤ç»„æ˜¯å¦å¯ç”¨äº†è‡ªåŠ¨è§£æ

        Args:
            group_id: ç¾¤ç»„ID

        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        if not self.user_manager:
            return False

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        "SELECT auto_parse_enabled FROM social_parser_config WHERE group_id = %s",
                        (group_id,)
                    )
                    result = await cursor.fetchone()
                    logger.debug(f"æŸ¥è¯¢ç»“æœ: result={result}, type={type(result)}")
                    if not result:
                        logger.debug(f"ç¾¤ç»„ {group_id} æ²¡æœ‰é…ç½®è®°å½•ï¼Œè¿”å›False")
                        return False

                    # å®‰å…¨åœ°è·å–å€¼
                    try:
                        if isinstance(result, (tuple, list)):
                            enabled = result[0]
                        elif isinstance(result, dict):
                            enabled = result.get('auto_parse_enabled', 0)
                        else:
                            logger.warning(f"æœªçŸ¥çš„resultç±»å‹: {type(result)}")
                            enabled = 0

                        logger.debug(f"ç¾¤ç»„ {group_id} auto_parse_enabledå€¼: {enabled}")
                        return bool(enabled)
                    except (IndexError, KeyError, TypeError) as e:
                        logger.error(f"è§£æresultå¤±è´¥: {e}, result={result}")
                        return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥è‡ªåŠ¨è§£æçŠ¶æ€å¤±è´¥: {e}", exc_info=True)
            return False

    async def enable_auto_parse(self, group_id: int, enabled_by: int) -> bool:
        """
        å¯ç”¨ç¾¤ç»„è‡ªåŠ¨è§£æ

        Args:
            group_id: ç¾¤ç»„ID
            enabled_by: å¯ç”¨è€…ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.user_manager:
            return False

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        INSERT INTO social_parser_config (group_id, auto_parse_enabled, enabled_by)
                        VALUES (%s, TRUE, %s)
                        ON DUPLICATE KEY UPDATE
                            auto_parse_enabled = TRUE,
                            enabled_by = %s,
                            updated_at = CURRENT_TIMESTAMP
                        """,
                        (group_id, enabled_by, enabled_by)
                    )
                    await conn.commit()
                    logger.info(f"ç¾¤ç»„ {group_id} å¯ç”¨è‡ªåŠ¨è§£æ")
                    return True
        except Exception as e:
            logger.error(f"å¯ç”¨è‡ªåŠ¨è§£æå¤±è´¥: {e}")
            return False

    async def disable_auto_parse(self, group_id: int) -> bool:
        """
        ç¦ç”¨ç¾¤ç»„è‡ªåŠ¨è§£æ

        Args:
            group_id: ç¾¤ç»„ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.user_manager:
            return False

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        UPDATE social_parser_config
                        SET auto_parse_enabled = FALSE,
                            updated_at = CURRENT_TIMESTAMP
                        WHERE group_id = %s
                        """,
                        (group_id,)
                    )
                    await conn.commit()
                    logger.info(f"ç¾¤ç»„ {group_id} ç¦ç”¨è‡ªåŠ¨è§£æ")
                    return True
        except Exception as e:
            logger.error(f"ç¦ç”¨è‡ªåŠ¨è§£æå¤±è´¥: {e}")
            return False

    async def get_auto_parse_groups(self) -> List[int]:
        """
        è·å–æ‰€æœ‰å¯ç”¨è‡ªåŠ¨è§£æçš„ç¾¤ç»„

        Returns:
            List[int]: ç¾¤ç»„IDåˆ—è¡¨
        """
        if not self.user_manager:
            return []

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        "SELECT group_id FROM social_parser_config WHERE auto_parse_enabled = TRUE"
                    )
                    results = await cursor.fetchall()
                    return [row[0] for row in results]
        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨è§£æç¾¤ç»„åˆ—è¡¨å¤±è´¥: {e}")
            return []

    # ==================== é«˜çº§åŠŸèƒ½ ====================

    async def upload_to_image_host(self, file_path: Path) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°å›¾åºŠ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            å›¾åºŠURLï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_image_host:
            return None

        try:
            from utils.image_host import ImageHostUploader

            proxy = self.config.downloader_proxy if self.config else None
            kwargs = {}
            if self.config.catbox_userhash:
                kwargs["catbox_userhash"] = self.config.catbox_userhash
            if self.config.zioooo_storage_id:
                kwargs["zioooo_storage_id"] = self.config.zioooo_storage_id

            async with ImageHostUploader(
                service=self.config.image_host_service,
                proxy=proxy,
                **kwargs
            ) as uploader:
                url = await uploader.upload(file_path)
                return url

        except Exception as e:
            logger.error(f"ä¸Šä¼ åˆ°å›¾åºŠå¤±è´¥: {e}")
            return None

    async def split_large_video(self, video_path: Path) -> List[Path]:
        """
        åˆ†å‰²å¤§è§†é¢‘æ–‡ä»¶

        Args:
            video_path: è§†é¢‘è·¯å¾„

        Returns:
            åˆ†å‰²åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¦‚æœä¸éœ€è¦åˆ†å‰²ï¼Œè¿”å›åŸæ–‡ä»¶ï¼‰
        """
        if not self.config or not self.config.enable_video_split:
            return [video_path]

        try:
            from utils.video_splitter import split_video_if_needed

            parts = await split_video_if_needed(
                video_path,
                max_size_mb=self.config.video_split_size,
                ffmpeg_path=self.config.ffmpeg_path
            )
            return parts

        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†å‰²å¤±è´¥: {e}")
            return [video_path]

    async def generate_ai_summary(self, download_result) -> Optional[str]:
        """
        ä½¿ç”¨ParseHubå†…ç½®çš„AIæ€»ç»“åŠŸèƒ½ç”Ÿæˆå†…å®¹æ€»ç»“

        Args:
            download_result: DownloadResult å¯¹è±¡

        Returns:
            æ€»ç»“æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_ai_summary:
            return None

        if not self.config.openai_api_key:
            logger.warning("AIæ€»ç»“åŠŸèƒ½å·²å¯ç”¨ä½†æœªé…ç½® OPENAI_API_KEY")
            return None

        try:
            # è‡ªå®šä¹‰ AI æ€»ç»“ promptï¼ˆç”ŸåŠ¨æœ‰è¶£çš„é£æ ¼ï¼‰
            custom_prompt = """ä½ æ˜¯ä¸€ä¸ªæ´»æ³¼å‹å¥½çš„ç¤¾äº¤åª’ä½“åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£è§†é¢‘/æ–‡ç« å†…å®¹ã€‚

è¯·ç”¨ç”ŸåŠ¨æœ‰è¶£çš„æ–¹å¼æ€»ç»“è¿™ä¸ªå†…å®¹ï¼Œè¦æ±‚ï¼š

**æ ¼å¼è¦æ±‚ï¼š**
- ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆç²—ä½“ã€å¼•ç”¨ã€åˆ—è¡¨ç­‰ï¼‰
- ä¸­è‹±æ–‡ä¹‹é—´éœ€è¦ç©ºæ ¼
- æŠ€æœ¯å…³é”®è¯ä½¿ç”¨ `è¡Œå†…ä»£ç `
- é€‚å½“ä½¿ç”¨ emoji è®©å†…å®¹æ›´å‹å¥½ï¼ˆä½†ä¸è¦è¿‡åº¦ï¼‰

**å†…å®¹ç»“æ„ï¼š**
1. **æ ¸å¿ƒå†…å®¹** - ç”¨ 1-2 å¥è¯è¯´æ˜ä¸»é¢˜ï¼ˆç”¨ç²—ä½“ï¼‰
2. **å…³é”®è¦ç‚¹** - 3-5 ä¸ªè¦ç‚¹ï¼Œä½¿ç”¨åˆ—è¡¨æ ¼å¼
3. **äº®ç‚¹/çœ‹ç‚¹** - å¦‚æœæœ‰è¶£çš„ç‰‡æ®µã€é‡‘å¥ã€æˆ–å€¼å¾—å…³æ³¨çš„ç»†èŠ‚ï¼Œç”¨å¼•ç”¨æ ¼å¼çªå‡ºæ˜¾ç¤º

**è¯­æ°”é£æ ¼ï¼š**
- ä¿æŒè½»æ¾å‹å¥½ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ ·
- å¯¹æœ‰è¶£çš„å†…å®¹å¯ä»¥åŠ ç‚¹ä¿çš®è¯„è®º
- é‡è¦ä¿¡æ¯è¦æ¸…æ™°å‡†ç¡®ï¼Œä¸å¤¸å¤§ä¸é—æ¼
- **å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›å¤**ï¼ˆå¦‚æœå†…å®¹æ˜¯è‹±æ–‡ï¼Œè¯·ç¿»è¯‘æˆä¸­æ–‡åå†æ€»ç»“ï¼‰

**æ³¨æ„äº‹é¡¹ï¼š**
- å¦‚æœæ˜¯è§†é¢‘ï¼Œå…³æ³¨è§†è§‰å†…å®¹å’Œå¯¹è¯
- å¦‚æœæ˜¯æ–‡ç« ï¼Œå…³æ³¨è®ºç‚¹å’Œè®ºæ®
- å¦‚æœæ˜¯ç¤¾äº¤åª’ä½“å¸–å­ï¼Œå…³æ³¨æƒ…ç»ªå’Œäº’åŠ¨
- æ€»é•¿åº¦æ§åˆ¶åœ¨ 200-500 å­—å·¦å³

ç°åœ¨è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š"""

            # ä½¿ç”¨ ParseHub å†…ç½®çš„ summary() æ–¹æ³•
            # æ³¨æ„ï¼šéœ€è¦ä¼ é€’å®Œæ•´çš„é…ç½®å‚æ•°
            # å¦‚æœæ²¡æœ‰é…ç½®è½¬å½•APIï¼Œä½¿ç”¨AIæ€»ç»“çš„APIï¼ˆWhisperå’ŒGPTå¯ä»¥ç”¨åŒä¸€ä¸ªAPI keyï¼‰
            transcription_api_key = self.config.transcription_api_key or self.config.openai_api_key
            transcription_base_url = self.config.transcription_base_url or self.config.openai_base_url

            summary_result = await download_result.summary(
                api_key=self.config.openai_api_key,
                base_url=self.config.openai_base_url,
                model=self.config.ai_summary_model,
                provider="openai",
                prompt=custom_prompt,  # æ·»åŠ è‡ªå®šä¹‰ prompt
                transcriptions_provider=self.config.transcription_provider or "openai",
                transcriptions_api_key=transcription_api_key,
                transcriptions_base_url=transcription_base_url,
            )

            # summary_result.content æ˜¯æ€»ç»“æ–‡æœ¬
            logger.info(f"âœ… AIæ€»ç»“ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary_result.content)}")
            return summary_result.content

        except Exception as e:
            logger.error(f"AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return None

    async def publish_to_telegraph(self, result: ParseResult, content_html: str) -> Optional[str]:
        """
        å‘å¸ƒå†…å®¹åˆ° Telegraph

        Args:
            result: è§£æç»“æœ
            content_html: HTMLå†…å®¹

        Returns:
            Telegraph URLï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_telegraph:
            return None

        try:
            from utils.telegraph_helper import TelegraphPublisher

            publisher = TelegraphPublisher(
                access_token=self.config.telegraph_token,
                author_name=self.config.telegraph_author
            )

            url = await publisher.create_page(
                title=result.title or "è§£æå†…å®¹",
                content=content_html
            )

            if url and not self.config.telegraph_token:
                # ä¿å­˜è‡ªåŠ¨åˆ›å»ºçš„token
                self.config.telegraph_token = publisher.access_token

            return url

        except Exception as e:
            logger.error(f"Telegraphå‘å¸ƒå¤±è´¥: {e}")
            return None

