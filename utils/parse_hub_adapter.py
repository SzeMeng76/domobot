"""
ParseHub é€‚é…å™¨
å°† ParseHub åº“é€‚é…åˆ° python-telegram-bot æ¡†æ¶
æ•´åˆæ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼šå›¾åºŠä¸Šä¼ ã€AIæ€»ç»“ã€Telegraphå‘å¸ƒã€è§†é¢‘åˆ†å‰²ã€è½¬å½•ç­‰
"""

import asyncio
import hashlib
import logging
import tempfile
import time
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any

# Apply monkey patch to fix Facebook/YouTube parsing (ParseHub 1.5.10 format bug)
from utils.parsehub_patch import patch_parsehub_yt_dlp
patch_parsehub_yt_dlp()

from parsehub import ParseHub
from parsehub.config import DownloadConfig, ParseConfig, GlobalConfig
from parsehub.types import ParseResult, VideoParseResult, ImageParseResult, MultimediaParseResult

logger = logging.getLogger(__name__)


class ParseHubAdapter:
    """ParseHub é€‚é…å™¨ç±»"""

    def __init__(self, cache_manager=None, user_manager=None, config=None):
        """
        åˆå§‹åŒ–é€‚é…å™¨

        Args:
            cache_manager: Redis ç¼“å­˜ç®¡ç†å™¨
            user_manager: MySQL ç”¨æˆ·ç®¡ç†å™¨
            config: Boté…ç½®å¯¹è±¡
        """
        self.cache_manager = cache_manager
        self.user_manager = user_manager
        self.config = config
        self.parsehub = ParseHub()
        self.temp_dir = Path(tempfile.gettempdir()) / "domobot_parse"
        self.temp_dir.mkdir(exist_ok=True)

        # é…ç½® ParseHub GlobalConfig
        if config:
            if config.douyin_api:
                GlobalConfig.douyin_api = config.douyin_api
                logger.info(f"âœ… é…ç½®æŠ–éŸ³API: {config.douyin_api}")
            GlobalConfig.duration_limit = 0  # ä¸é™åˆ¶è§†é¢‘æ—¶é•¿

            # é…ç½®ä¼ªè£…User-Agentï¼ˆç»•è¿‡åçˆ¬è™«ï¼‰
            GlobalConfig.ua = (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            )
            logger.info(f"âœ… é…ç½®ä¼ªè£…User-Agent: Chrome/131.0.0.0")

    async def get_supported_platforms(self) -> List[str]:
        """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
        try:
            platforms = self.parsehub.get_supported_platforms()
            return platforms
        except Exception as e:
            logger.error(f"è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def check_url_supported(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ”¯æŒçš„å¹³å°URL

        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬

        Returns:
            bool: æ˜¯å¦åŒ…å«æ”¯æŒçš„URL
        """
        try:
            # æå– URL
            url = await self._extract_url(text)
            if not url:
                return False

            # æ£€æŸ¥æ˜¯å¦æ”¯æŒ
            parser = self.parsehub.select_parser(url)
            return parser is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥URLæ”¯æŒå¤±è´¥: {e}")
            return False

    async def parse_url(
        self,
        text: str,
        user_id: int,
        group_id: Optional[int] = None,
        proxy: Optional[str] = None
    ) -> Tuple[Optional[Any], Optional[str], float]:
        """
        è§£æURLå¹¶ä¸‹è½½åª’ä½“

        Args:
            text: åŒ…å«URLçš„æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„IDï¼ˆå¯é€‰ï¼‰
            proxy: ä»£ç†åœ°å€ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ä»£ç†ï¼‰

        Returns:
            (DownloadResult, platform_name, parse_time): ä¸‹è½½ç»“æœã€å¹³å°åç§°ã€è§£æè€—æ—¶
        """
        start_time = time.time()

        try:
            # æå– URL
            url = await self._extract_url(text)
            if not url:
                return None, None, 0

            # æ³¨æ„ï¼šDownloadResultåŒ…å«æ–‡ä»¶å¯¹è±¡ï¼Œä¸èƒ½åºåˆ—åŒ–åˆ°Redisç¼“å­˜
            # æ¯æ¬¡éƒ½éœ€è¦é‡æ–°è§£æå’Œä¸‹è½½
            # cache_key = self._get_cache_key(url)

            # é€‰æ‹©è§£æå™¨
            parser = self.parsehub.select_parser(url)
            if not parser:
                logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {url}")
                return None, None, 0

            # è·å–å¹³å°ID
            platform_id = getattr(parser, '__platform_id__', '')
            platform_name = platform_id or "unknown"

            # é…ç½®ä»£ç†ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„proxyï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„proxyï¼‰
            parser_proxy = proxy or (self.config.parser_proxy if self.config else None)
            downloader_proxy = proxy or (self.config.downloader_proxy if self.config else None)

            # æ ¹æ®å¹³å°é€‰æ‹© Cookieï¼ˆParseConfigä¼šè‡ªåŠ¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºdictï¼‰
            # æ³¨æ„ï¼šåªæœ‰éƒ¨åˆ†å¹³å°æ”¯æŒcookieï¼ˆTwitter, Instagram, Bilibili, Kuaishouï¼‰
            # Facebook/YouTubeç­‰åŸºäºyt-dlpçš„å¹³å°ä¸æ”¯æŒcookieï¼ˆParseHubåº“é™åˆ¶ï¼‰
            platform_cookie = None
            if self.config:
                logger.info(f"ğŸ” å¹³å°: {platform_id}")
                if platform_id == 'twitter' and self.config.twitter_cookie:
                    platform_cookie = self.config.twitter_cookie
                    logger.info(f"âœ… ä½¿ç”¨Twitter cookie: {platform_cookie[:50]}...")
                elif platform_id == 'instagram' and self.config.instagram_cookie:
                    platform_cookie = self.config.instagram_cookie
                    logger.info(f"âœ… ä½¿ç”¨Instagram cookie")
                elif platform_id == 'bilibili' and self.config.bilibili_cookie:
                    platform_cookie = self.config.bilibili_cookie
                    logger.info(f"âœ… ä½¿ç”¨Bilibili cookie: {platform_cookie[:50]}...")
                elif platform_id == 'kuaishou' and self.config.kuaishou_cookie:
                    platform_cookie = self.config.kuaishou_cookie
                    logger.info(f"âœ… ä½¿ç”¨Kuaishou cookie")
                else:
                    logger.info(f"âš ï¸ å¹³å° {platform_id} æœªé…ç½®cookieæˆ–ä¸åŒ¹é…")

            # é…ç½®å¹³å°ç‰¹å®šçš„headersï¼ˆç”¨äºç»•è¿‡åçˆ¬è™«ï¼‰
            download_headers = {
                'User-Agent': GlobalConfig.ua,
            }

            # æ ¹æ®å¹³å°æ·»åŠ  Referer å’Œ Originï¼ˆç±»ä¼¼å†…ç½®ä»£ç†çš„ç­–ç•¥ï¼‰
            if platform_id == 'bilibili':
                download_headers.update({
                    'Referer': 'https://www.bilibili.com/',
                    'Origin': 'https://www.bilibili.com',
                })
                logger.info(f"âœ… é…ç½®Bilibili headers: Referer + Origin")
            elif platform_id == 'youtube':
                download_headers.update({
                    'Referer': 'https://www.youtube.com/',
                    'Origin': 'https://www.youtube.com',
                })
                logger.info(f"âœ… é…ç½®YouTube headers: Referer + Origin")
            elif platform_id == 'twitter':
                download_headers.update({
                    'Referer': 'https://twitter.com/',
                    'Origin': 'https://twitter.com',
                })
                logger.info(f"âœ… é…ç½®Twitter headers: Referer + Origin")
            elif platform_id == 'tiktok':
                download_headers.update({
                    'Referer': 'https://www.tiktok.com/',
                    'Origin': 'https://www.tiktok.com',
                })
                logger.info(f"âœ… é…ç½®TikTok headers: Referer + Origin")
            elif platform_id == 'xiaohongshu':
                download_headers.update({
                    'Referer': 'https://www.xiaohongshu.com/',
                    'Origin': 'https://www.xiaohongshu.com',
                })
                logger.info(f"âœ… é…ç½®å°çº¢ä¹¦ headers: Referer + Origin")

            # åˆ›å»ºé…ç½®ï¼ˆé‡è¦ï¼šæ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„ParseHubå®ä¾‹ï¼Œä¼ å…¥é…ç½®ï¼‰
            parse_config = ParseConfig(proxy=parser_proxy, cookie=platform_cookie)
            logger.info(f"ParseConfig cookie type: {type(parse_config.cookie)}, value: {parse_config.cookie}")
            download_config = DownloadConfig(
                proxy=downloader_proxy,
                save_dir=self.temp_dir,
                headers=download_headers  # ä¼ å…¥å¹³å°ç‰¹å®šheaders
            )

            # åˆ›å»ºæ–°çš„ParseHubå®ä¾‹å¹¶ä¼ å…¥é…ç½®
            parsehub = ParseHub(config=parse_config)
            result = await parsehub.parse(url)

            if not result:
                return None, None, 0

            # ä¸‹è½½åª’ä½“
            try:
                download_result = await result.download(config=download_config)
            except Exception as download_error:
                # ä¸‹è½½å¤±è´¥ï¼ˆä¾‹å¦‚å°çº¢ä¹¦CDN 500é”™è¯¯ï¼‰ï¼Œä½†è§£ææˆåŠŸ
                # è¿”å›è§£æç»“æœä½†æ²¡æœ‰ä¸‹è½½çš„åª’ä½“æ–‡ä»¶
                logger.warning(f"åª’ä½“ä¸‹è½½å¤±è´¥ä½†è§£ææˆåŠŸ: {download_error}")
                # åˆ›å»ºä¸€ä¸ªç©ºçš„DownloadResultï¼ˆåªåŒ…å«è§£æä¿¡æ¯ï¼Œæ²¡æœ‰å®é™…æ–‡ä»¶ï¼‰
                from parsehub.types import DownloadResult
                download_result = DownloadResult(parse_result=result, media=None)

            parse_time = time.time() - start_time

            # æ³¨æ„ï¼šDownloadResultä¸èƒ½åºåˆ—åŒ–ï¼Œä¸ç¼“å­˜åˆ°Redis
            # ParseHubè‡ªå·±æœ‰å†…å­˜ç¼“å­˜æœºåˆ¶å¤„ç†é‡å¤URL

            # è®°å½•ç»Ÿè®¡
            await self._record_stats(user_id, group_id, platform_name, url, True, parse_time * 1000)

            return download_result, platform_name, parse_time

        except Exception as e:
            parse_time = time.time() - start_time
            logger.error(f"è§£æURLå¤±è´¥: {e}", exc_info=True)

            # è®°å½•å¤±è´¥ç»Ÿè®¡
            await self._record_stats(
                user_id,
                group_id,
                "unknown",
                text,
                False,
                parse_time * 1000,
                str(e)
            )

            return None, None, parse_time

    async def format_result(self, download_result, platform: str) -> dict:
        """
        æ ¼å¼åŒ–ä¸‹è½½ç»“æœ

        Args:
            download_result: DownloadResult ä¸‹è½½ç»“æœ
            platform: å¹³å°åç§°

        Returns:
            dict: æ ¼å¼åŒ–åçš„ç»“æœ
        """
        try:
            # download_result.pr æ˜¯åŸå§‹çš„ ParseResult
            pr = download_result.pr
            formatted = {
                "title": pr.title or "æ— æ ‡é¢˜",
                "desc": pr.desc or "",
                "platform": platform,
                "url": pr.raw_url,
                "media_type": self._get_media_type(pr),
                "media_count": 0,
                "media_paths": []
            }

            # è·å–åª’ä½“è·¯å¾„
            media = download_result.media
            if isinstance(media, list):
                formatted["media_count"] = len(media)
                formatted["media_paths"] = [str(m.path) for m in media if m.exists()]
            elif media and media.exists():
                formatted["media_count"] = 1
                formatted["media_paths"] = [str(media.path)]

            return formatted

        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–ç»“æœå¤±è´¥: {e}")
            return {}

    def _get_media_type(self, result: ParseResult) -> str:
        """è·å–åª’ä½“ç±»å‹"""
        if isinstance(result, VideoParseResult):
            return "video"
        elif isinstance(result, ImageParseResult):
            return "image"
        elif isinstance(result, MultimediaParseResult):
            return "multimedia"
        else:
            return "unknown"

    async def _extract_url(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–URL"""
        import re
        import httpx

        try:
            # 1. å…ˆä»æ–‡æœ¬ä¸­æå–URL
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
            match = re.search(url_pattern, text)
            if not match:
                return None

            url = match.group(0)
            logger.debug(f"æå–åŸå§‹URL: {url}")

            # 2. æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­é“¾æ¥ï¼ˆéœ€è¦é‡å®šå‘ï¼‰
            short_domains = ['b23.tv', 't.co', 'bit.ly', 'youtu.be', 'tiktok.com/t/', 'douyin.com/']
            is_short = any(domain in url for domain in short_domains)

            if is_short:
                # è·Ÿéšé‡å®šå‘è·å–çœŸå®URL
                try:
                    async with httpx.AsyncClient(follow_redirects=True, timeout=10) as client:
                        response = await client.head(url)
                        final_url = str(response.url)
                        logger.info(f"çŸ­é“¾æ¥é‡å®šå‘: {url} -> {final_url}")
                        url = final_url
                except Exception as e:
                    logger.warning(f"é‡å®šå‘å¤±è´¥ï¼Œä½¿ç”¨åŸURL: {e}")

            # 3. éªŒè¯URLæ˜¯å¦è¢«æ”¯æŒ
            parser = self.parsehub.select_parser(url)
            if not parser:
                logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {url}")
                return None

            logger.debug(f"è¯†åˆ«å¹³å°: {parser.__platform__}")
            return url

        except Exception as e:
            logger.error(f"æå–URLå¤±è´¥: {text}, é”™è¯¯: {e}", exc_info=True)
            return None

    def _get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return f"parse:result:{url_hash}"

    async def _record_stats(
        self,
        user_id: int,
        group_id: Optional[int],
        platform: str,
        url: str,
        success: bool,
        parse_time_ms: float,
        error_message: Optional[str] = None
    ):
        """è®°å½•è§£æç»Ÿè®¡"""
        if not self.user_manager:
            return

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        INSERT INTO social_parser_stats
                        (user_id, group_id, platform, url, parse_success, parse_time_ms, error_message)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """,
                        (user_id, group_id, platform, url, success, int(parse_time_ms), error_message)
                    )
                    await conn.commit()
        except Exception as e:
            logger.error(f"è®°å½•è§£æç»Ÿè®¡å¤±è´¥: {e}")

    async def cleanup_temp_files(self, older_than_hours: int = 24):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            cutoff_time = time.time() - (older_than_hours * 3600)

            for item in self.temp_dir.iterdir():
                if item.is_dir() and item.stat().st_mtime < cutoff_time:
                    shutil.rmtree(item, ignore_errors=True)
                    logger.info(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {item}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    async def is_auto_parse_enabled(self, group_id: int) -> bool:
        """
        æ£€æŸ¥ç¾¤ç»„æ˜¯å¦å¯ç”¨äº†è‡ªåŠ¨è§£æ

        Args:
            group_id: ç¾¤ç»„ID

        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        if not self.user_manager:
            return False

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        "SELECT auto_parse_enabled FROM social_parser_config WHERE group_id = %s",
                        (group_id,)
                    )
                    result = await cursor.fetchone()
                    return bool(result[0]) if result else False
        except Exception as e:
            logger.error(f"æ£€æŸ¥è‡ªåŠ¨è§£æçŠ¶æ€å¤±è´¥: {e}")
            return False

    async def enable_auto_parse(self, group_id: int, enabled_by: int) -> bool:
        """
        å¯ç”¨ç¾¤ç»„è‡ªåŠ¨è§£æ

        Args:
            group_id: ç¾¤ç»„ID
            enabled_by: å¯ç”¨è€…ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.user_manager:
            return False

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        INSERT INTO social_parser_config (group_id, auto_parse_enabled, enabled_by)
                        VALUES (%s, TRUE, %s)
                        ON DUPLICATE KEY UPDATE
                            auto_parse_enabled = TRUE,
                            enabled_by = %s,
                            updated_at = CURRENT_TIMESTAMP
                        """,
                        (group_id, enabled_by, enabled_by)
                    )
                    await conn.commit()
                    logger.info(f"ç¾¤ç»„ {group_id} å¯ç”¨è‡ªåŠ¨è§£æ")
                    return True
        except Exception as e:
            logger.error(f"å¯ç”¨è‡ªåŠ¨è§£æå¤±è´¥: {e}")
            return False

    async def disable_auto_parse(self, group_id: int) -> bool:
        """
        ç¦ç”¨ç¾¤ç»„è‡ªåŠ¨è§£æ

        Args:
            group_id: ç¾¤ç»„ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.user_manager:
            return False

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        """
                        UPDATE social_parser_config
                        SET auto_parse_enabled = FALSE,
                            updated_at = CURRENT_TIMESTAMP
                        WHERE group_id = %s
                        """,
                        (group_id,)
                    )
                    await conn.commit()
                    logger.info(f"ç¾¤ç»„ {group_id} ç¦ç”¨è‡ªåŠ¨è§£æ")
                    return True
        except Exception as e:
            logger.error(f"ç¦ç”¨è‡ªåŠ¨è§£æå¤±è´¥: {e}")
            return False

    async def get_auto_parse_groups(self) -> List[int]:
        """
        è·å–æ‰€æœ‰å¯ç”¨è‡ªåŠ¨è§£æçš„ç¾¤ç»„

        Returns:
            List[int]: ç¾¤ç»„IDåˆ—è¡¨
        """
        if not self.user_manager:
            return []

        try:
            async with self.user_manager.pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        "SELECT group_id FROM social_parser_config WHERE auto_parse_enabled = TRUE"
                    )
                    results = await cursor.fetchall()
                    return [row[0] for row in results]
        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨è§£æç¾¤ç»„åˆ—è¡¨å¤±è´¥: {e}")
            return []

    # ==================== é«˜çº§åŠŸèƒ½ ====================

    async def upload_to_image_host(self, file_path: Path) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°å›¾åºŠ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            å›¾åºŠURLï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_image_host:
            return None

        try:
            from utils.image_host import ImageHostUploader

            proxy = self.config.downloader_proxy if self.config else None
            kwargs = {}
            if self.config.catbox_userhash:
                kwargs["catbox_userhash"] = self.config.catbox_userhash
            if self.config.zioooo_storage_id:
                kwargs["zioooo_storage_id"] = self.config.zioooo_storage_id

            async with ImageHostUploader(
                service=self.config.image_host_service,
                proxy=proxy,
                **kwargs
            ) as uploader:
                url = await uploader.upload(file_path)
                return url

        except Exception as e:
            logger.error(f"ä¸Šä¼ åˆ°å›¾åºŠå¤±è´¥: {e}")
            return None

    async def split_large_video(self, video_path: Path) -> List[Path]:
        """
        åˆ†å‰²å¤§è§†é¢‘æ–‡ä»¶

        Args:
            video_path: è§†é¢‘è·¯å¾„

        Returns:
            åˆ†å‰²åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¦‚æœä¸éœ€è¦åˆ†å‰²ï¼Œè¿”å›åŸæ–‡ä»¶ï¼‰
        """
        if not self.config or not self.config.enable_video_split:
            return [video_path]

        try:
            from utils.video_splitter import split_video_if_needed

            parts = await split_video_if_needed(
                video_path,
                max_size_mb=self.config.video_split_size,
                ffmpeg_path=self.config.ffmpeg_path
            )
            return parts

        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†å‰²å¤±è´¥: {e}")
            return [video_path]

    async def generate_ai_summary(self, result: ParseResult) -> Optional[str]:
        """
        ä½¿ç”¨AIç”Ÿæˆå†…å®¹æ€»ç»“

        Args:
            result: è§£æç»“æœ

        Returns:
            æ€»ç»“æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_ai_summary:
            return None

        if not self.config.openai_api_key:
            logger.warning("AIæ€»ç»“åŠŸèƒ½å·²å¯ç”¨ä½†æœªé…ç½® OPENAI_API_KEY")
            return None

        try:
            from utils.ai_summary import generate_summary

            summary = await generate_summary(
                result=result,
                api_key=self.config.openai_api_key,
                model=self.config.ai_summary_model,
                base_url=self.config.openai_base_url,
                max_length=50
            )

            return summary

        except Exception as e:
            logger.error(f"AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            return None

    async def publish_to_telegraph(self, result: ParseResult, content_html: str) -> Optional[str]:
        """
        å‘å¸ƒå†…å®¹åˆ° Telegraph

        Args:
            result: è§£æç»“æœ
            content_html: HTMLå†…å®¹

        Returns:
            Telegraph URLï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_telegraph:
            return None

        try:
            from utils.telegraph_helper import TelegraphPublisher

            publisher = TelegraphPublisher(
                access_token=self.config.telegraph_token,
                author_name=self.config.telegraph_author
            )

            url = await publisher.create_page(
                title=result.title or "è§£æå†…å®¹",
                content=content_html
            )

            if url and not self.config.telegraph_token:
                # ä¿å­˜è‡ªåŠ¨åˆ›å»ºçš„token
                self.config.telegraph_token = publisher.access_token

            return url

        except Exception as e:
            logger.error(f"Telegraphå‘å¸ƒå¤±è´¥: {e}")
            return None

    async def transcribe_video(self, video_path: Path) -> Optional[str]:
        """
        è½¬å½•è§†é¢‘éŸ³é¢‘ä¸ºæ–‡å­—

        Args:
            video_path: è§†é¢‘è·¯å¾„

        Returns:
            è½¬å½•æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.config or not self.config.enable_transcription:
            return None

        if not self.config.transcription_api_key:
            logger.warning("è½¬å½•åŠŸèƒ½å·²å¯ç”¨ä½†æœªé…ç½® API KEY")
            return None

        try:
            from utils.transcription import transcribe_media

            # æ ¹æ®é…ç½®é€‰æ‹©è½¬å½•æœåŠ¡
            kwargs = {
                "api_key": self.config.transcription_api_key,
            }

            if self.config.transcription_provider == "openai":
                kwargs["base_url"] = self.config.transcription_base_url
            elif self.config.transcription_provider == "azure":
                # Azure éœ€è¦ region
                kwargs["region"] = self.config.transcription_base_url or "eastus"

            text = await transcribe_media(
                media_path=video_path,
                provider=self.config.transcription_provider,
                **kwargs
            )

            return text

        except Exception as e:
            logger.error(f"è§†é¢‘è½¬å½•å¤±è´¥: {e}")
            return None
